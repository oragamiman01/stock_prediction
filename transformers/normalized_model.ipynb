{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from StockDataWrapper import get_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthDataset(Dataset):\n",
    "    def __init__(self, tickers: list[str], data_dir: str, lag_days: int, test_days: int, test: bool):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.tickers = []\n",
    "\n",
    "        num_to_ticker = dict()\n",
    "        ticker_to_num = dict()\n",
    "        ids = np.linspace(-1, 1, len(tickers)).round(5)\n",
    "        for i, num in enumerate(ids):\n",
    "            num_to_ticker[str(num)] = tickers[i]\n",
    "            ticker_to_num[tickers[i]] = num\n",
    "\n",
    "        # get the unix timestamp value for 2024-04-15. Used to scale the date value used as input to the model\n",
    "        cur_unix_timestamp = datetime.datetime(year=2024, month=4, day=15).timestamp()\n",
    "\n",
    "        for ticker in tickers:\n",
    "            ordered_time_series = get_time_series(ticker, data_dir, normalize=True)\n",
    "\n",
    "            if test:\n",
    "                idx = range(len(ordered_time_series) - lag_days - test_days, len(ordered_time_series))\n",
    "            else:\n",
    "                idx = range(lag_days, len(ordered_time_series) - lag_days - test_days)\n",
    "\n",
    "            # need to get lag_days days of input tokens and predict the lag_days+1 day, and so on\n",
    "            for cur_target_idx in idx:\n",
    "\n",
    "                # get target values\n",
    "                cur_target_actual_after = [ticker_to_num[ticker]]\n",
    "                for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                    val = ordered_time_series[cur_target_idx][key]\n",
    "                    if key == 'date':\n",
    "                        val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                    cur_target_actual_after.append(val)\n",
    "\n",
    "                # each \"sample\" (input) will have multiple tokens. Each token is a vector of the day's values\n",
    "                cur_input = []\n",
    "\n",
    "                for i in range(cur_target_idx - lag_days, cur_target_idx):\n",
    "\n",
    "                    cur_token = [ticker_to_num[ticker]]\n",
    "                    for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                        val = ordered_time_series[cur_target_idx][key]\n",
    "                        if key == 'date':\n",
    "                            val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                        cur_token.append(val)\n",
    "\n",
    "                    cur_input.append(cur_token)\n",
    "\n",
    "                self.X.append(cur_input)\n",
    "                self.y.append(cur_target_actual_after)\n",
    "                self.tickers.append(ticker)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32), self.tickers[idx]\n",
    "    \n",
    "\n",
    "class IncrementalDataset(Dataset):\n",
    "    def __init__(self, tickers: list[str], data_dir: str, lag_days: int, test_days: int, test: bool):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.tickers = []\n",
    "\n",
    "        num_to_ticker = dict()\n",
    "        ticker_to_num = dict()\n",
    "        ids = np.linspace(-1, 1, len(tickers)).round(5)\n",
    "        for i, num in enumerate(ids):\n",
    "            num_to_ticker[str(num)] = tickers[i]\n",
    "            ticker_to_num[tickers[i]] = num\n",
    "\n",
    "        # get the unix timestamp value for 2024-04-15. Used to scale the date value used as input to the model\n",
    "        cur_unix_timestamp = datetime.datetime(year=2024, month=4, day=15).timestamp()\n",
    "\n",
    "        for ticker in tickers:\n",
    "            ordered_time_series = get_time_series(ticker, data_dir, normalize=True)\n",
    "\n",
    "            if test:\n",
    "                idx = range(len(ordered_time_series) - lag_days - test_days, len(ordered_time_series) - lag_days - test_days + 1)\n",
    "            else:\n",
    "                idx = range(lag_days + 1500, len(ordered_time_series) - lag_days - test_days) # CUT OFF OLD PART OF DATA!!!!!!!!!!!!! FYI\n",
    "\n",
    "            # need to get lag_days days of input tokens and predict the lag_days+1 day, and so on\n",
    "            for cur_target_idx in idx:\n",
    "\n",
    "                # get target values\n",
    "                cur_target_actual_after = [ticker_to_num[ticker]]\n",
    "                for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                    val = ordered_time_series[cur_target_idx][key]\n",
    "                    if key == 'date':\n",
    "                        val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                    cur_target_actual_after.append(val)\n",
    "\n",
    "                # each \"sample\" (input) will have multiple tokens. Each token is a vector of the day's values\n",
    "                cur_input = []\n",
    "\n",
    "                for i in range(cur_target_idx - lag_days, cur_target_idx):\n",
    "\n",
    "                    cur_token = [ticker_to_num[ticker]]\n",
    "                    for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                        val = ordered_time_series[cur_target_idx][key]\n",
    "                        if key == 'date':\n",
    "                            val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                        cur_token.append(val)\n",
    "\n",
    "                    cur_input.append(cur_token)\n",
    "\n",
    "                self.X.append(cur_input)\n",
    "                self.y.append(cur_target_actual_after)\n",
    "                self.tickers.append(ticker)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32), self.tickers[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer (batch first)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class BigTransformer(nn.Module):\n",
    "    def __init__(self, indim, outdim, hidden_dim=256, d_model=64, nhead=4, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super(BigTransformer, self).__init__()\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(indim, hidden_dim),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "            )\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, batch_first=True)\n",
    "\n",
    "        self.decoder_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, outdim)\n",
    "        )\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "    # src: (B, S, F) batches, sequence length, features\n",
    "    # tgt: (B, F)\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.embedding(tgt.unsqueeze(1))\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src.shape[1]).to(device)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.shape[1]).to(device)\n",
    "        out = self.transformer(src, tgt, src_mask, tgt_mask, src_is_causal=True, tgt_is_causal=True)\n",
    "        out = self.decoder_mlp(out)\n",
    "        return out.squeeze(1)\n",
    "    \n",
    "    def generate(self, src, max_len=100, start_token=None, end_token=None):\n",
    "        device = src.device\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src.shape[1]).to(device)\n",
    "\n",
    "        output_seq = torch.zeros((src.shape[0], 1, self.d_model)).to(device)\n",
    "        if start_token is not None:\n",
    "            output_seq[:, 0] = self.embedding(start_token.to(device))\n",
    "\n",
    "        for i in range(max_len):\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(output_seq.shape[1]).to(device)\n",
    "            next_token = self.transformer(src, output_seq, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "            output_seq = torch.cat([output_seq, next_token], dim=1)\n",
    "\n",
    "            if end_token is not None and torch.all(next_token == end_token):\n",
    "                break\n",
    "            \n",
    "        output_seq = self.decoder_mlp(output_seq[:, 1:]) # exclude start token\n",
    "        \n",
    "        return output_seq.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# big transformer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public methods are constructor, train, evaluate, and load_model\n",
    "class BigTransformerAgent:\n",
    "    def __init__(self, indim, outdim, hidden_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 device, checkpoint_dir, init_lr, lr_decay, min_lr, decay_lr_every):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        self.allow_training = True\n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.device = device\n",
    "        self.model = BigTransformer(indim, outdim, hidden_dim, d_model, nhead, \n",
    "                                    num_encoder_layers, num_decoder_layers)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=init_lr)\n",
    "\n",
    "        # at epoch e, lr will be init_lr * scheduler_lamb(e)\n",
    "        scheduler_lamb = lambda epoch: max(lr_decay ** (epoch // decay_lr_every), min_lr / init_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, scheduler_lamb)\n",
    "\n",
    "        self.criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cur_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        self.allow_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def __save_model(self):\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "\n",
    "        dt = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(self.checkpoint_dir, f\"{dt}_agent_{self.cur_epoch}.pth\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"epoch\": self.cur_epoch\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def __save_returns(self, returns):\n",
    "        returns_prefix = f\"{self.checkpoint_dir}/returns\"\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        if not os.path.isdir(returns_prefix):\n",
    "            os.mkdir(returns_prefix)\n",
    "\n",
    "        np_filename = f\"{returns_prefix}/{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.npy\"\n",
    "        np.save(np_filename, returns)\n",
    "\n",
    "        return np_filename\n",
    "\n",
    "    def train(self, dataloader, epochs):\n",
    "        if not self.allow_training:\n",
    "            raise Exception(\"Not allowed to train after loading\")\n",
    "\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for (X, y, tickers) in dataloader:\n",
    "                X = X.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                y_pred = self.model(X, y)\n",
    "\n",
    "                loss = self.criterion(y, y_pred)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # TODO is this good?\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # self.scheduler.step()\n",
    "\n",
    "            losses.append(epoch_loss)\n",
    "\n",
    "            self.cur_epoch += 1\n",
    "\n",
    "            if self.cur_epoch % 25 == 0:\n",
    "                self.__save_model()\n",
    "\n",
    "            average_loss = epoch_loss / len(dataloader.dataset)  # epoch loss is the sum of each sample's loss since mse reduction is sum\n",
    "            print(f\"Epoch {self.cur_epoch}, Loss: {epoch_loss:.4f}; Average Loss: {average_loss}; lr: {self.scheduler.get_last_lr()}\")\n",
    "\n",
    "        self.model.eval()\n",
    "        return self.__save_returns(losses)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        gt = torch.Tensor([])\n",
    "        preds = torch.Tensor([])\n",
    "        companies = []\n",
    "        for batch, (X, y, co) in enumerate(dataloader):\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            y_pred = self.model.generate(X, max_len=1)\n",
    "\n",
    "            loss = self.criterion(y, y_pred)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            gt = torch.concat([gt, y], dim=0)\n",
    "            preds = torch.concat([preds, y_pred], dim=0)\n",
    "            companies.extend(co)\n",
    "\n",
    "        print(f\"Average Loss: {total_loss / len(dataloader.dataset)}\")  # total loss is the sum of each sample's loss since mse reduction is sum\n",
    "\n",
    "        gt = gt.to('cpu')\n",
    "        preds = preds.to('cpu')\n",
    "\n",
    "        return gt, preds, companies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# incremental transformer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public methods are constructor, train, evaluate, and load_model\n",
    "# agent for incremental learning models\n",
    "class IncrementalAgent:\n",
    "    def __init__(self, indim, outdim, hidden_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 device, checkpoint_dir, init_lr, lr_decay, min_lr, decay_lr_every, tickers):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.tickers = tickers\n",
    "\n",
    "        self.allow_training = True\n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.device = device\n",
    "        self.model = BigTransformer(indim, outdim, hidden_dim, d_model, nhead, \n",
    "                                    num_encoder_layers, num_decoder_layers)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=init_lr)\n",
    "\n",
    "        # at epoch e, lr will be init_lr * scheduler_lamb(e)\n",
    "        scheduler_lamb = lambda epoch: max(lr_decay ** (epoch // decay_lr_every), min_lr / init_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, scheduler_lamb)\n",
    "\n",
    "        self.criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cur_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        self.allow_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def __save_model(self):\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "\n",
    "        dt = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(self.checkpoint_dir, f\"{dt}_agent_{self.cur_epoch}.pth\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"epoch\": self.cur_epoch\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def __save_returns(self, fn, returns):\n",
    "        returns_prefix = f\"{self.checkpoint_dir}/returns\"\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        if not os.path.isdir(returns_prefix):\n",
    "            os.mkdir(returns_prefix)\n",
    "\n",
    "        np_filename = f\"{returns_prefix}/{fn}{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.npy\"\n",
    "        np.save(np_filename, returns)\n",
    "\n",
    "        return np_filename\n",
    "\n",
    "\n",
    "    def train(self, test_days, epochs):\n",
    "        if not self.allow_training:\n",
    "            raise Exception(\"Not allowed to train after loading\")\n",
    "        \n",
    "\n",
    "        losses = []\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        companies = []\n",
    "\n",
    "        for day in range(test_days):\n",
    "            # create new datasets for each new test_day\n",
    "            train_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, False)\n",
    "            test_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, True)\n",
    "            train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "            test_loader = DataLoader(test_set)\n",
    "\n",
    "            losses_per_day = []\n",
    "            self.model.train()\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for (X, y, tickers) in train_loader:\n",
    "                    X = X.to(self.device)\n",
    "                    y = y.to(self.device)\n",
    "\n",
    "                    y_pred = self.model(X, y)\n",
    "\n",
    "                    loss = self.criterion(y_pred, y)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # TODO is this good?\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                # self.scheduler.step()\n",
    "\n",
    "                losses_per_day.append(epoch_loss)\n",
    "\n",
    "                self.cur_epoch += 1\n",
    "\n",
    "                if self.cur_epoch % 25 == 0:\n",
    "                    self.__save_model()\n",
    "\n",
    "                average_loss = epoch_loss / len(train_set)  # epoch loss is the sum of each sample's loss since mse reduction is sum\n",
    "                print(f\"Test Day {day}, Epoch {self.cur_epoch}, Loss: {epoch_loss:.4f}; Average Loss: {average_loss}; lr: {self.scheduler.get_last_lr()}\")\n",
    "\n",
    "            gt, pred, tickers = self.evaluate(test_loader)\n",
    "            ground_truths.append(gt)\n",
    "            predictions.append(pred)\n",
    "            companies.extend(tickers)\n",
    "\n",
    "            losses.append(losses_per_day)\n",
    "\n",
    "        ground_truths = np.array(ground_truths)\n",
    "        predictions = np.array(ground_truths)\n",
    "\n",
    "        self.__save_returns('ground_truths_', ground_truths)\n",
    "        self.__save_returns('predictions', predictions)\n",
    "\n",
    "        self.model.eval()\n",
    "        return ground_truths, predictions, companies\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        gt = torch.Tensor([])\n",
    "        preds = torch.Tensor([])\n",
    "        companies = []\n",
    "        for batch, (X, y, co) in enumerate(dataloader):\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            y_pred = self.model.generate(X, max_len=1)\n",
    "\n",
    "            loss = self.criterion(y_pred, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            gt = torch.concat([gt, y], dim=0)\n",
    "            preds = torch.concat([preds, y_pred], dim=0)\n",
    "            companies.extend(co)\n",
    "\n",
    "        print(f\"Test Loss: {total_loss / len(dataloader.dataset)}\")  # total loss is the sum of each sample's loss since mse reduction is sum\n",
    "\n",
    "        gt = gt.to('cpu').detach().numpy()\n",
    "        preds = preds.to('cpu').detach().numpy()\n",
    "\n",
    "        return gt, preds, companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Probability Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public methods are constructor, train, evaluate, and load_model\n",
    "# agent for incremental learning models\n",
    "class IncrementalProbAgent:\n",
    "    def __init__(self, indim, outdim, hidden_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 device, checkpoint_dir, init_lr, lr_decay, min_lr, decay_lr_every, tickers):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.tickers = tickers\n",
    "\n",
    "        self.allow_training = True\n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.device = device\n",
    "        self.model = BigTransformer(indim, outdim, hidden_dim, d_model, nhead, \n",
    "                                    num_encoder_layers, num_decoder_layers)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=init_lr)\n",
    "\n",
    "        # at epoch e, lr will be init_lr * scheduler_lamb(e)\n",
    "        scheduler_lamb = lambda epoch: max(lr_decay ** (epoch // decay_lr_every), min_lr / init_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, scheduler_lamb)\n",
    "\n",
    "        weight = torch.tensor([0.75]).to(device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction='sum', pos_weight=weight)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cur_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        self.allow_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def __save_model(self):\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "\n",
    "        dt = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(self.checkpoint_dir, f\"{dt}_agent_{self.cur_epoch}.pth\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"epoch\": self.cur_epoch\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def __save_returns(self, fn, returns):\n",
    "        returns_prefix = f\"{self.checkpoint_dir}/returns\"\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        if not os.path.isdir(returns_prefix):\n",
    "            os.mkdir(returns_prefix)\n",
    "\n",
    "        np_filename = f\"{returns_prefix}/{fn}{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.npy\"\n",
    "        np.save(np_filename, returns)\n",
    "\n",
    "        return np_filename\n",
    "\n",
    "    def train(self, test_days, epochs):\n",
    "        if not self.allow_training:\n",
    "            raise Exception(\"Not allowed to train after loading\")\n",
    "        \n",
    "        losses = []\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        companies = []\n",
    "\n",
    "        for day in range(test_days):\n",
    "            # create new datasets for each new test_day\n",
    "            train_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, False)\n",
    "            test_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, True)\n",
    "            train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "            test_loader = DataLoader(test_set)\n",
    "\n",
    "            losses_per_day = []\n",
    "            self.model.train()\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for (X, y, tickers) in train_loader:\n",
    "                    X = X.to(self.device)\n",
    "                    y = y.to(self.device)\n",
    "\n",
    "                    y_pred = self.model(X, y) # y_pred will be (B, 1)\n",
    "\n",
    "                    y = y[:,7].clone() # pct_change is 7th column\n",
    "                    y[y <= 0] = 0\n",
    "                    y[y > 0] = 1\n",
    "\n",
    "                    loss = self.criterion(y_pred, y.unsqueeze(1))\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # TODO is this good?\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                # self.scheduler.step()\n",
    "\n",
    "                losses_per_day.append(epoch_loss)\n",
    "\n",
    "                self.cur_epoch += 1\n",
    "\n",
    "                if self.cur_epoch % 25 == 0:\n",
    "                    self.__save_model()\n",
    "\n",
    "                average_loss = epoch_loss / len(train_set)  # epoch loss is the sum of each sample's loss since mse reduction is sum\n",
    "                print(f\"Test Day {day}, Epoch {self.cur_epoch}, Loss: {epoch_loss:.4f}; Average Loss: {average_loss}; lr: {self.scheduler.get_last_lr()}\")\n",
    "\n",
    "            gt, pred, tickers = self.evaluate(test_loader)\n",
    "            ground_truths.append(gt)\n",
    "            predictions.append(pred)\n",
    "            companies.extend(tickers)\n",
    "\n",
    "            losses.append(losses_per_day)\n",
    "\n",
    "        ground_truths = np.array(ground_truths).squeeze()\n",
    "        predictions = np.array(predictions).squeeze()\n",
    "\n",
    "        self.__save_returns('ground_truths_', ground_truths)\n",
    "        self.__save_returns('predictions', predictions)\n",
    "\n",
    "        self.model.eval()\n",
    "        return ground_truths, predictions, companies\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        gt = torch.Tensor([]).to(self.device)\n",
    "        preds = torch.Tensor([]).to(self.device)\n",
    "        companies = []\n",
    "        for batch, (X, y, co) in enumerate(dataloader):\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            y_pred = self.model.generate(X, max_len=1)\n",
    "\n",
    "            y = y[:,7].clone() # pct_change is 7th column\n",
    "            y[y <= 0] = 0\n",
    "            y[y > 0] = 1\n",
    "\n",
    "            loss = self.criterion(y_pred, y.unsqueeze(1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            gt = torch.concat([gt, y.unsqueeze(1)], dim=0)\n",
    "            preds = torch.concat([preds, torch.sigmoid(y_pred)], dim=0)\n",
    "            companies.extend(co)\n",
    "\n",
    "        print(f\"Test Loss: {total_loss / len(dataloader.dataset)}\")  # total loss is the sum of each sample's loss since mse reduction is sum\n",
    "\n",
    "        gt = gt.to('cpu').detach().numpy()\n",
    "        preds = preds.to('cpu').detach().numpy()\n",
    "\n",
    "        return gt, preds, companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "tickers = [\"AAPL\"]\n",
    "data_dir = \"../data_combined/\"\n",
    "test_days = 100\n",
    "lag_days = 30\n",
    "batch_size = 128\n",
    "\n",
    "# model params\n",
    "indim = 23\n",
    "outdim = 1\n",
    "hidden_dim = 256\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = VariableLengthDataset(tickers=tickers, data_dir=data_dir, lag_days=lag_days, test_days=test_days, test=False)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = VariableLengthDataset(tickers=tickers, data_dir=data_dir, lag_days=lag_days, test_days=test_days, test=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BigTransformerAgent(indim=23, outdim=23, hidden_dim=256, d_model=64, nhead=8, num_encoder_layers=6, num_decoder_layers=6, device=device, \n",
    "                            checkpoint_dir='../saved_models/23outdimMSEmodel', init_lr=0.001, lr_decay=0.0001, min_lr=0.0000025, decay_lr_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(train_loader, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = IncrementalAgent(indim=23, outdim=23, hidden_dim=256, d_model=64, nhead=8, num_encoder_layers=6, num_decoder_layers=6, device=device, \n",
    "                            checkpoint_dir='../saved_models/23outdimIncrementalMSEmodel', init_lr=0.0001, lr_decay=0.5, min_lr=0.00000025, decay_lr_every=10, \n",
    "                            tickers=tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, preds, companies = agent.train(test_days, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = IncrementalProbAgent(indim=23, outdim=1, hidden_dim=256, d_model=64, nhead=8, num_encoder_layers=6, num_decoder_layers=6, device=device, \n",
    "                            checkpoint_dir='../saved_models/incrementalBCE_AAPL', init_lr=0.0001, lr_decay=0.5, min_lr=0.00000025, decay_lr_every=10, \n",
    "                            tickers=tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Day 0, Epoch 1, Loss: 465.7565; Average Loss: 0.6017525849108241; lr: [0.0001]\n",
      "Test Day 0, Epoch 2, Loss: 465.7785; Average Loss: 0.6017810488240047; lr: [0.0001]\n",
      "Test Day 0, Epoch 3, Loss: 466.7027; Average Loss: 0.6029750247334325; lr: [0.0001]\n",
      "Test Day 0, Epoch 4, Loss: 465.8919; Average Loss: 0.601927554576588; lr: [0.0001]\n",
      "Test Day 0, Epoch 5, Loss: 466.4434; Average Loss: 0.6026401113170062; lr: [0.0001]\n",
      "Test Day 0, Epoch 6, Loss: 465.0092; Average Loss: 0.6007870919328635; lr: [0.0001]\n",
      "Test Day 0, Epoch 7, Loss: 466.8204; Average Loss: 0.603127186304531; lr: [0.0001]\n",
      "Test Day 0, Epoch 8, Loss: 466.8270; Average Loss: 0.6031356252133077; lr: [0.0001]\n",
      "Test Day 0, Epoch 9, Loss: 466.5988; Average Loss: 0.6028408800908761; lr: [0.0001]\n",
      "Test Day 0, Epoch 10, Loss: 466.1216; Average Loss: 0.6022242495563911; lr: [0.0001]\n",
      "Test Day 0, Epoch 11, Loss: 465.6758; Average Loss: 0.6016482832511882; lr: [0.0001]\n",
      "Test Day 0, Epoch 12, Loss: 466.2190; Average Loss: 0.6023501930926813; lr: [0.0001]\n",
      "Test Day 0, Epoch 13, Loss: 465.8256; Average Loss: 0.6018418218430315; lr: [0.0001]\n",
      "Test Day 0, Epoch 14, Loss: 465.3447; Average Loss: 0.6012205178115411; lr: [0.0001]\n",
      "Test Day 0, Epoch 15, Loss: 465.2219; Average Loss: 0.6010618296088482; lr: [0.0001]\n",
      "Test Loss: 0.6859475374221802\n",
      "Test Day 1, Epoch 16, Loss: 466.9103; Average Loss: 0.6024648604854461; lr: [0.0001]\n",
      "Test Day 1, Epoch 17, Loss: 467.8100; Average Loss: 0.6036257663849861; lr: [0.0001]\n",
      "Test Day 1, Epoch 18, Loss: 467.8030; Average Loss: 0.6036167883103893; lr: [0.0001]\n",
      "Test Day 1, Epoch 19, Loss: 465.1625; Average Loss: 0.6002096449944281; lr: [0.0001]\n",
      "Test Day 1, Epoch 20, Loss: 468.1016; Average Loss: 0.6040020708883962; lr: [0.0001]\n",
      "Test Day 1, Epoch 21, Loss: 466.7818; Average Loss: 0.6022990780491982; lr: [0.0001]\n",
      "Test Day 1, Epoch 22, Loss: 467.1813; Average Loss: 0.6028146325388263; lr: [0.0001]\n",
      "Test Day 1, Epoch 23, Loss: 467.9808; Average Loss: 0.6038461875915527; lr: [0.0001]\n",
      "Test Day 1, Epoch 24, Loss: 466.5179; Average Loss: 0.60195862923899; lr: [0.0001]\n",
      "Test Day 1, Epoch 25, Loss: 467.6993; Average Loss: 0.6034829053571147; lr: [0.0001]\n",
      "Test Day 1, Epoch 26, Loss: 466.4908; Average Loss: 0.6019236706149194; lr: [0.0001]\n",
      "Test Day 1, Epoch 27, Loss: 466.7689; Average Loss: 0.6022824287414551; lr: [0.0001]\n",
      "Test Day 1, Epoch 28, Loss: 466.7546; Average Loss: 0.6022639526859407; lr: [0.0001]\n",
      "Test Day 1, Epoch 29, Loss: 467.6901; Average Loss: 0.6034710748734012; lr: [0.0001]\n",
      "Test Day 1, Epoch 30, Loss: 467.8134; Average Loss: 0.6036301840505293; lr: [0.0001]\n",
      "Test Loss: 0.5273672342300415\n",
      "Test Day 2, Epoch 31, Loss: 466.3797; Average Loss: 0.601004831569711; lr: [0.0001]\n",
      "Test Day 2, Epoch 32, Loss: 467.7964; Average Loss: 0.6028304653069407; lr: [0.0001]\n",
      "Test Day 2, Epoch 33, Loss: 467.7855; Average Loss: 0.6028164065990251; lr: [0.0001]\n",
      "Test Day 2, Epoch 34, Loss: 467.5731; Average Loss: 0.60254264123661; lr: [0.0001]\n",
      "Test Day 2, Epoch 35, Loss: 468.1431; Average Loss: 0.6032772310001334; lr: [0.0001]\n",
      "Test Day 2, Epoch 36, Loss: 465.4859; Average Loss: 0.5998529171206287; lr: [0.0001]\n",
      "Test Day 2, Epoch 37, Loss: 468.1276; Average Loss: 0.6032571571389425; lr: [0.0001]\n",
      "Test Day 2, Epoch 38, Loss: 468.0192; Average Loss: 0.603117482563884; lr: [0.0001]\n",
      "Test Day 2, Epoch 39, Loss: 466.7764; Average Loss: 0.6015159297235233; lr: [0.0001]\n",
      "Test Day 2, Epoch 40, Loss: 467.1948; Average Loss: 0.6020551698724019; lr: [0.0001]\n",
      "Test Day 2, Epoch 41, Loss: 467.2245; Average Loss: 0.6020934225357685; lr: [0.0001]\n",
      "Test Day 2, Epoch 42, Loss: 466.4462; Average Loss: 0.6010904717691166; lr: [0.0001]\n",
      "Test Day 2, Epoch 43, Loss: 467.3548; Average Loss: 0.6022613607731062; lr: [0.0001]\n",
      "Test Day 2, Epoch 44, Loss: 466.6888; Average Loss: 0.6014030317670291; lr: [0.0001]\n",
      "Test Day 2, Epoch 45, Loss: 467.5963; Average Loss: 0.6025724945609102; lr: [0.0001]\n",
      "Test Loss: 0.530704140663147\n",
      "Test Day 3, Epoch 46, Loss: 468.8005; Average Loss: 0.6033467865972139; lr: [0.0001]\n",
      "Test Day 3, Epoch 47, Loss: 468.7550; Average Loss: 0.6032883448950572; lr: [0.0001]\n",
      "Test Day 3, Epoch 48, Loss: 467.2003; Average Loss: 0.6012874392162112; lr: [0.0001]\n",
      "Test Day 3, Epoch 49, Loss: 467.6258; Average Loss: 0.6018349869677766; lr: [0.0001]\n",
      "Test Day 3, Epoch 50, Loss: 467.3199; Average Loss: 0.60144127535237; lr: [0.0001]\n",
      "Test Day 3, Epoch 51, Loss: 466.6927; Average Loss: 0.6006340802559674; lr: [0.0001]\n",
      "Test Day 3, Epoch 52, Loss: 465.9756; Average Loss: 0.5997111677663206; lr: [0.0001]\n",
      "Test Day 3, Epoch 53, Loss: 465.9099; Average Loss: 0.5996266325775107; lr: [0.0001]\n",
      "Test Day 3, Epoch 54, Loss: 468.0157; Average Loss: 0.60233685101935; lr: [0.0001]\n",
      "Test Day 3, Epoch 55, Loss: 467.0702; Average Loss: 0.6011198981541618; lr: [0.0001]\n",
      "Test Day 3, Epoch 56, Loss: 466.8443; Average Loss: 0.6008291747732666; lr: [0.0001]\n",
      "Test Day 3, Epoch 57, Loss: 466.2506; Average Loss: 0.600065109812615; lr: [0.0001]\n",
      "Test Day 3, Epoch 58, Loss: 466.8900; Average Loss: 0.6008880607870094; lr: [0.0001]\n",
      "Test Day 3, Epoch 59, Loss: 467.9908; Average Loss: 0.6023048084214847; lr: [0.0001]\n",
      "Test Day 3, Epoch 60, Loss: 466.7830; Average Loss: 0.6007502640536393; lr: [0.0001]\n",
      "Test Loss: 0.6749375462532043\n",
      "Test Day 4, Epoch 61, Loss: 467.3318; Average Loss: 0.6006835916661358; lr: [0.0001]\n",
      "Test Day 4, Epoch 62, Loss: 468.0634; Average Loss: 0.6016239537677912; lr: [0.0001]\n",
      "Test Day 4, Epoch 63, Loss: 467.3169; Average Loss: 0.6006643962124626; lr: [0.0001]\n",
      "Test Day 4, Epoch 64, Loss: 468.2352; Average Loss: 0.6018447618558045; lr: [0.0001]\n",
      "Test Day 4, Epoch 65, Loss: 467.6290; Average Loss: 0.6010655032944863; lr: [0.0001]\n",
      "Test Day 4, Epoch 66, Loss: 467.4076; Average Loss: 0.6007809565428903; lr: [0.0001]\n",
      "Test Day 4, Epoch 67, Loss: 468.0607; Average Loss: 0.6016204988435485; lr: [0.0001]\n",
      "Test Day 4, Epoch 68, Loss: 467.2124; Average Loss: 0.6005300212948366; lr: [0.0001]\n",
      "Test Day 4, Epoch 69, Loss: 468.4040; Average Loss: 0.6020617239885894; lr: [0.0001]\n",
      "Test Day 4, Epoch 70, Loss: 468.4121; Average Loss: 0.6020720991806383; lr: [0.0001]\n",
      "Test Day 4, Epoch 71, Loss: 467.6160; Average Loss: 0.6010487986714184; lr: [0.0001]\n",
      "Test Day 4, Epoch 72, Loss: 467.9396; Average Loss: 0.601464758188988; lr: [0.0001]\n",
      "Test Day 4, Epoch 73, Loss: 467.4597; Average Loss: 0.6008479533894203; lr: [0.0001]\n",
      "Test Day 4, Epoch 74, Loss: 466.6131; Average Loss: 0.5997598324459439; lr: [0.0001]\n",
      "Test Day 4, Epoch 75, Loss: 466.7890; Average Loss: 0.5999859219038701; lr: [0.0001]\n",
      "Test Loss: 0.6707398891448975\n",
      "Test Day 5, Epoch 76, Loss: 468.3837; Average Loss: 0.6012627812190907; lr: [0.0001]\n",
      "Test Day 5, Epoch 77, Loss: 467.9223; Average Loss: 0.6006704337789709; lr: [0.0001]\n",
      "Test Day 5, Epoch 78, Loss: 468.5126; Average Loss: 0.6014282528947039; lr: [0.0001]\n",
      "Test Day 5, Epoch 79, Loss: 468.3225; Average Loss: 0.6011841526692577; lr: [0.0001]\n",
      "Test Day 5, Epoch 80, Loss: 468.7953; Average Loss: 0.601791107608663; lr: [0.0001]\n",
      "Test Day 5, Epoch 81, Loss: 469.4507; Average Loss: 0.6026325127891766; lr: [0.0001]\n",
      "Test Day 5, Epoch 82, Loss: 468.4957; Average Loss: 0.6014065540494906; lr: [0.0001]\n",
      "Test Day 5, Epoch 83, Loss: 469.3941; Average Loss: 0.6025597856347766; lr: [0.0001]\n",
      "Test Day 5, Epoch 84, Loss: 466.8848; Average Loss: 0.5993385957661581; lr: [0.0001]\n",
      "Test Day 5, Epoch 85, Loss: 467.3578; Average Loss: 0.5999458494174159; lr: [0.0001]\n",
      "Test Day 5, Epoch 86, Loss: 467.9800; Average Loss: 0.60074458654793; lr: [0.0001]\n",
      "Test Day 5, Epoch 87, Loss: 467.5825; Average Loss: 0.6002342471415333; lr: [0.0001]\n",
      "Test Day 5, Epoch 88, Loss: 468.7703; Average Loss: 0.6017590064904503; lr: [0.0001]\n",
      "Test Day 5, Epoch 89, Loss: 466.9648; Average Loss: 0.5994413067348808; lr: [0.0001]\n",
      "Test Day 5, Epoch 90, Loss: 467.9664; Average Loss: 0.6007270880320871; lr: [0.0001]\n",
      "Test Loss: 0.6684327721595764\n",
      "Test Day 6, Epoch 91, Loss: 467.7042; Average Loss: 0.5996207762987186; lr: [0.0001]\n",
      "Test Day 6, Epoch 92, Loss: 468.9429; Average Loss: 0.6012089099639502; lr: [0.0001]\n",
      "Test Day 6, Epoch 93, Loss: 469.2933; Average Loss: 0.6016580459399101; lr: [0.0001]\n",
      "Test Day 6, Epoch 94, Loss: 468.6903; Average Loss: 0.600885053170033; lr: [0.0001]\n",
      "Test Day 6, Epoch 95, Loss: 469.0598; Average Loss: 0.6013587817167625; lr: [0.0001]\n",
      "Test Day 6, Epoch 96, Loss: 469.7596; Average Loss: 0.6022558517945118; lr: [0.0001]\n",
      "Test Day 6, Epoch 97, Loss: 466.9614; Average Loss: 0.5986684456849709; lr: [0.0001]\n",
      "Test Day 6, Epoch 98, Loss: 468.4980; Average Loss: 0.6006384421617557; lr: [0.0001]\n",
      "Test Day 6, Epoch 99, Loss: 468.0424; Average Loss: 0.6000542983030661; lr: [0.0001]\n",
      "Test Day 6, Epoch 100, Loss: 468.8246; Average Loss: 0.6010572109466944; lr: [0.0001]\n",
      "Test Day 6, Epoch 101, Loss: 467.6503; Average Loss: 0.5995516654772636; lr: [0.0001]\n",
      "Test Day 6, Epoch 102, Loss: 468.7976; Average Loss: 0.6010226029616136; lr: [0.0001]\n",
      "Test Day 6, Epoch 103, Loss: 467.8947; Average Loss: 0.5998649719433907; lr: [0.0001]\n",
      "Test Day 6, Epoch 104, Loss: 467.5666; Average Loss: 0.5994443820073054; lr: [0.0001]\n",
      "Test Day 6, Epoch 105, Loss: 467.9179; Average Loss: 0.599894737585997; lr: [0.0001]\n",
      "Test Loss: 0.668432354927063\n",
      "Test Day 7, Epoch 106, Loss: 468.4703; Average Loss: 0.5998338847093179; lr: [0.0001]\n",
      "Test Day 7, Epoch 107, Loss: 468.6215; Average Loss: 0.6000275569139156; lr: [0.0001]\n",
      "Test Day 7, Epoch 108, Loss: 468.8728; Average Loss: 0.600349262032405; lr: [0.0001]\n",
      "Test Day 7, Epoch 109, Loss: 469.2440; Average Loss: 0.6008246039611582; lr: [0.0001]\n",
      "Test Day 7, Epoch 110, Loss: 469.3768; Average Loss: 0.6009946333461473; lr: [0.0001]\n",
      "Test Day 7, Epoch 111, Loss: 469.5283; Average Loss: 0.601188657225781; lr: [0.0001]\n",
      "Test Day 7, Epoch 112, Loss: 468.9800; Average Loss: 0.6004865007730208; lr: [0.0001]\n",
      "Test Day 7, Epoch 113, Loss: 469.5209; Average Loss: 0.6011791314724618; lr: [0.0001]\n",
      "Test Day 7, Epoch 114, Loss: 468.7617; Average Loss: 0.6002070516080771; lr: [0.0001]\n",
      "Test Day 7, Epoch 115, Loss: 467.7898; Average Loss: 0.5989625975966606; lr: [0.0001]\n",
      "Test Day 7, Epoch 116, Loss: 468.2499; Average Loss: 0.5995517528011307; lr: [0.0001]\n",
      "Test Day 7, Epoch 117, Loss: 469.5235; Average Loss: 0.6011824210993612; lr: [0.0001]\n",
      "Test Day 7, Epoch 118, Loss: 471.2085; Average Loss: 0.6033399956784999; lr: [0.0001]\n",
      "Test Day 7, Epoch 119, Loss: 468.7609; Average Loss: 0.6002060014673446; lr: [0.0001]\n",
      "Test Day 7, Epoch 120, Loss: 468.9278; Average Loss: 0.600419678852897; lr: [0.0001]\n",
      "Test Loss: 0.662038266658783\n",
      "Test Day 8, Epoch 121, Loss: 470.5368; Average Loss: 0.60170942926041; lr: [0.0001]\n",
      "Test Day 8, Epoch 122, Loss: 471.7272; Average Loss: 0.6032317520102577; lr: [0.0001]\n",
      "Test Day 8, Epoch 123, Loss: 470.3759; Average Loss: 0.601503747808354; lr: [0.0001]\n",
      "Test Day 8, Epoch 124, Loss: 470.6459; Average Loss: 0.6018489242514686; lr: [0.0001]\n",
      "Test Day 8, Epoch 125, Loss: 470.4666; Average Loss: 0.6016197277761787; lr: [0.0001]\n",
      "Test Day 8, Epoch 126, Loss: 469.7252; Average Loss: 0.6006716572110306; lr: [0.0001]\n",
      "Test Day 8, Epoch 127, Loss: 468.8970; Average Loss: 0.5996125750529492; lr: [0.0001]\n",
      "Test Day 8, Epoch 128, Loss: 468.8860; Average Loss: 0.5995984382336706; lr: [0.0001]\n",
      "Test Day 8, Epoch 129, Loss: 470.3180; Average Loss: 0.6014296075572139; lr: [0.0001]\n",
      "Test Day 8, Epoch 130, Loss: 469.3178; Average Loss: 0.6001506156628699; lr: [0.0001]\n",
      "Test Day 8, Epoch 131, Loss: 469.5177; Average Loss: 0.6004062052577963; lr: [0.0001]\n",
      "Test Day 8, Epoch 132, Loss: 468.7279; Average Loss: 0.5993963068403552; lr: [0.0001]\n",
      "Test Day 8, Epoch 133, Loss: 468.0136; Average Loss: 0.5984828685555617; lr: [0.0001]\n",
      "Test Day 8, Epoch 134, Loss: 468.4259; Average Loss: 0.599010132157894; lr: [0.0001]\n",
      "Test Day 8, Epoch 135, Loss: 469.7918; Average Loss: 0.6007568013027805; lr: [0.0001]\n",
      "Test Loss: 0.547048807144165\n",
      "Test Day 9, Epoch 136, Loss: 469.6594; Average Loss: 0.5998204354277668; lr: [0.0001]\n",
      "Test Day 9, Epoch 137, Loss: 471.0977; Average Loss: 0.6016573010733302; lr: [0.0001]\n",
      "Test Day 9, Epoch 138, Loss: 469.5414; Average Loss: 0.5996696781533584; lr: [0.0001]\n",
      "Test Day 9, Epoch 139, Loss: 470.0425; Average Loss: 0.6003096825318318; lr: [0.0001]\n",
      "Test Day 9, Epoch 140, Loss: 471.4904; Average Loss: 0.6021588570313435; lr: [0.0001]\n",
      "Test Day 9, Epoch 141, Loss: 469.4040; Average Loss: 0.5994942629778827; lr: [0.0001]\n",
      "Test Day 9, Epoch 142, Loss: 469.1337; Average Loss: 0.5991490828123129; lr: [0.0001]\n",
      "Test Day 9, Epoch 143, Loss: 471.7163; Average Loss: 0.6024473989299126; lr: [0.0001]\n",
      "Test Day 9, Epoch 144, Loss: 470.8869; Average Loss: 0.6013880993975838; lr: [0.0001]\n",
      "Test Day 9, Epoch 145, Loss: 470.4262; Average Loss: 0.6007997432431041; lr: [0.0001]\n",
      "Test Day 9, Epoch 146, Loss: 469.5482; Average Loss: 0.5996783951994408; lr: [0.0001]\n",
      "Test Day 9, Epoch 147, Loss: 469.3089; Average Loss: 0.5993728174864865; lr: [0.0001]\n",
      "Test Day 9, Epoch 148, Loss: 472.0316; Average Loss: 0.6028500382775396; lr: [0.0001]\n",
      "Test Day 9, Epoch 149, Loss: 470.3456; Average Loss: 0.6006968146235245; lr: [0.0001]\n",
      "Test Day 9, Epoch 150, Loss: 470.5613; Average Loss: 0.6009722425867619; lr: [0.0001]\n",
      "Test Loss: 0.6552107930183411\n",
      "Test Day 10, Epoch 151, Loss: 470.7527; Average Loss: 0.6004498090062823; lr: [0.0001]\n",
      "Test Day 10, Epoch 152, Loss: 470.6314; Average Loss: 0.6002952128040547; lr: [0.0001]\n",
      "Test Day 10, Epoch 153, Loss: 471.1432; Average Loss: 0.6009479590824672; lr: [0.0001]\n",
      "Test Day 10, Epoch 154, Loss: 472.0433; Average Loss: 0.6020960637501308; lr: [0.0001]\n",
      "Test Day 10, Epoch 155, Loss: 470.3288; Average Loss: 0.5999091729825857; lr: [0.0001]\n",
      "Test Day 10, Epoch 156, Loss: 471.2040; Average Loss: 0.6010254524192031; lr: [0.0001]\n",
      "Test Day 10, Epoch 157, Loss: 470.9230; Average Loss: 0.6006671555188238; lr: [0.0001]\n",
      "Test Day 10, Epoch 158, Loss: 469.9299; Average Loss: 0.5994003354286661; lr: [0.0001]\n",
      "Test Day 10, Epoch 159, Loss: 471.0878; Average Loss: 0.6008773458247282; lr: [0.0001]\n",
      "Test Day 10, Epoch 160, Loss: 469.5942; Average Loss: 0.5989722402728334; lr: [0.0001]\n",
      "Test Day 10, Epoch 161, Loss: 470.9488; Average Loss: 0.6006999478048208; lr: [0.0001]\n",
      "Test Day 10, Epoch 162, Loss: 470.2120; Average Loss: 0.5997602282738199; lr: [0.0001]\n",
      "Test Day 10, Epoch 163, Loss: 470.2767; Average Loss: 0.5998426700124935; lr: [0.0001]\n",
      "Test Day 10, Epoch 164, Loss: 470.5156; Average Loss: 0.6001474236955449; lr: [0.0001]\n",
      "Test Day 10, Epoch 165, Loss: 470.3325; Average Loss: 0.599913908510792; lr: [0.0001]\n",
      "Test Loss: 0.6534520387649536\n",
      "Test Day 11, Epoch 166, Loss: 472.7358; Average Loss: 0.6022112658069392; lr: [0.0001]\n",
      "Test Day 11, Epoch 167, Loss: 470.8389; Average Loss: 0.5997948069481334; lr: [0.0001]\n",
      "Test Day 11, Epoch 168, Loss: 470.8483; Average Loss: 0.5998066956829873; lr: [0.0001]\n",
      "Test Day 11, Epoch 169, Loss: 470.8827; Average Loss: 0.599850536759492; lr: [0.0001]\n",
      "Test Day 11, Epoch 170, Loss: 469.9062; Average Loss: 0.5986065931380934; lr: [0.0001]\n",
      "Test Day 11, Epoch 171, Loss: 471.5530; Average Loss: 0.6007044312300955; lr: [0.0001]\n",
      "Test Day 11, Epoch 172, Loss: 470.9342; Average Loss: 0.5999162017919455; lr: [0.0001]\n",
      "Test Day 11, Epoch 173, Loss: 471.2755; Average Loss: 0.6003509229915157; lr: [0.0001]\n",
      "Test Day 11, Epoch 174, Loss: 469.6870; Average Loss: 0.598327384025428; lr: [0.0001]\n",
      "Test Day 11, Epoch 175, Loss: 472.0671; Average Loss: 0.6013593831639381; lr: [0.0001]\n",
      "Test Day 11, Epoch 176, Loss: 472.2879; Average Loss: 0.6016406903600996; lr: [0.0001]\n",
      "Test Day 11, Epoch 177, Loss: 471.3898; Average Loss: 0.6004965861132191; lr: [0.0001]\n",
      "Test Day 11, Epoch 178, Loss: 471.2114; Average Loss: 0.600269341924388; lr: [0.0001]\n",
      "Test Day 11, Epoch 179, Loss: 470.7431; Average Loss: 0.599672691685379; lr: [0.0001]\n",
      "Test Day 11, Epoch 180, Loss: 471.2229; Average Loss: 0.6002838948729691; lr: [0.0001]\n",
      "Test Loss: 0.6511931419372559\n",
      "Test Day 12, Epoch 181, Loss: 471.5345; Average Loss: 0.5999166316355155; lr: [0.0001]\n",
      "Test Day 12, Epoch 182, Loss: 470.1185; Average Loss: 0.5981151808915854; lr: [0.0001]\n",
      "Test Day 12, Epoch 183, Loss: 470.8926; Average Loss: 0.5990999830891461; lr: [0.0001]\n",
      "Test Day 12, Epoch 184, Loss: 472.5072; Average Loss: 0.6011541453936627; lr: [0.0001]\n",
      "Test Day 12, Epoch 185, Loss: 470.8694; Average Loss: 0.5990704567984467; lr: [0.0001]\n",
      "Test Day 12, Epoch 186, Loss: 472.1862; Average Loss: 0.6007457750140862; lr: [0.0001]\n",
      "Test Day 12, Epoch 187, Loss: 471.6480; Average Loss: 0.6000610344282543; lr: [0.0001]\n",
      "Test Day 12, Epoch 188, Loss: 470.8837; Average Loss: 0.5990887404094822; lr: [0.0001]\n",
      "Test Day 12, Epoch 189, Loss: 471.6625; Average Loss: 0.6000794854782919; lr: [0.0001]\n",
      "Test Day 12, Epoch 190, Loss: 472.2782; Average Loss: 0.6008628415697403; lr: [0.0001]\n",
      "Test Day 12, Epoch 191, Loss: 472.4195; Average Loss: 0.6010425715652737; lr: [0.0001]\n",
      "Test Day 12, Epoch 192, Loss: 472.6543; Average Loss: 0.6013413142915294; lr: [0.0001]\n",
      "Test Day 12, Epoch 193, Loss: 469.8792; Average Loss: 0.597810700346192; lr: [0.0001]\n",
      "Test Day 12, Epoch 194, Loss: 470.6472; Average Loss: 0.5987877967096771; lr: [0.0001]\n",
      "Test Day 12, Epoch 195, Loss: 470.9008; Average Loss: 0.5991104043470388; lr: [0.0001]\n",
      "Test Loss: 0.5549055933952332\n",
      "Test Day 13, Epoch 196, Loss: 472.7032; Average Loss: 0.6006394038509384; lr: [0.0001]\n",
      "Test Day 13, Epoch 197, Loss: 472.2121; Average Loss: 0.6000153130770033; lr: [0.0001]\n",
      "Test Day 13, Epoch 198, Loss: 471.2504; Average Loss: 0.5987933933204738; lr: [0.0001]\n",
      "Test Day 13, Epoch 199, Loss: 471.7296; Average Loss: 0.599402307706685; lr: [0.0001]\n",
      "Test Day 13, Epoch 200, Loss: 471.8957; Average Loss: 0.5996133496528053; lr: [0.0001]\n",
      "Test Day 13, Epoch 201, Loss: 472.0190; Average Loss: 0.5997700697282067; lr: [0.0001]\n",
      "Test Day 13, Epoch 202, Loss: 472.3106; Average Loss: 0.6001405316053533; lr: [0.0001]\n",
      "Test Day 13, Epoch 203, Loss: 472.0924; Average Loss: 0.5998632850307674; lr: [0.0001]\n",
      "Test Day 13, Epoch 204, Loss: 472.2017; Average Loss: 0.6000021821813329; lr: [0.0001]\n",
      "Test Day 13, Epoch 205, Loss: 470.5497; Average Loss: 0.5979030432210338; lr: [0.0001]\n",
      "Test Day 13, Epoch 206, Loss: 470.6884; Average Loss: 0.5980792281891218; lr: [0.0001]\n",
      "Test Day 13, Epoch 207, Loss: 472.0382; Average Loss: 0.5997943708524013; lr: [0.0001]\n",
      "Test Day 13, Epoch 208, Loss: 472.0207; Average Loss: 0.599772161268067; lr: [0.0001]\n",
      "Test Day 13, Epoch 209, Loss: 472.7035; Average Loss: 0.6006397843512374; lr: [0.0001]\n",
      "Test Day 13, Epoch 210, Loss: 472.2202; Average Loss: 0.6000256423271596; lr: [0.0001]\n",
      "Test Loss: 0.6469986438751221\n",
      "Test Day 14, Epoch 211, Loss: 472.3450; Average Loss: 0.5994226521041792; lr: [0.0001]\n",
      "Test Day 14, Epoch 212, Loss: 473.2892; Average Loss: 0.600620832540057; lr: [0.0001]\n",
      "Test Day 14, Epoch 213, Loss: 473.3813; Average Loss: 0.6007376273876519; lr: [0.0001]\n",
      "Test Day 14, Epoch 214, Loss: 472.3500; Average Loss: 0.5994289151303054; lr: [0.0001]\n",
      "Test Day 14, Epoch 215, Loss: 473.5263; Average Loss: 0.6009217446225549; lr: [0.0001]\n",
      "Test Day 14, Epoch 216, Loss: 472.6002; Average Loss: 0.5997464898879152; lr: [0.0001]\n",
      "Test Day 14, Epoch 217, Loss: 470.9564; Average Loss: 0.5976603672589142; lr: [0.0001]\n",
      "Test Day 14, Epoch 218, Loss: 472.5209; Average Loss: 0.5996458373093968; lr: [0.0001]\n",
      "Test Day 14, Epoch 219, Loss: 472.8313; Average Loss: 0.6000396878586203; lr: [0.0001]\n",
      "Test Day 14, Epoch 220, Loss: 472.6131; Average Loss: 0.5997627737558433; lr: [0.0001]\n",
      "Test Day 14, Epoch 221, Loss: 473.1526; Average Loss: 0.6004475240174889; lr: [0.0001]\n",
      "Test Day 14, Epoch 222, Loss: 473.6738; Average Loss: 0.6011088281718607; lr: [0.0001]\n",
      "Test Day 14, Epoch 223, Loss: 471.5484; Average Loss: 0.598411684713993; lr: [0.0001]\n",
      "Test Day 14, Epoch 224, Loss: 472.8734; Average Loss: 0.6000930851486128; lr: [0.0001]\n",
      "Test Day 14, Epoch 225, Loss: 473.3581; Average Loss: 0.600708303112669; lr: [0.0001]\n",
      "Test Loss: 0.6445164084434509\n",
      "Test Day 15, Epoch 226, Loss: 473.6384; Average Loss: 0.6003021474724638; lr: [0.0001]\n",
      "Test Day 15, Epoch 227, Loss: 474.1853; Average Loss: 0.6009952680541838; lr: [0.0001]\n",
      "Test Day 15, Epoch 228, Loss: 474.2837; Average Loss: 0.6011199794039225; lr: [0.0001]\n",
      "Test Day 15, Epoch 229, Loss: 472.5417; Average Loss: 0.598912167760658; lr: [0.0001]\n",
      "Test Day 15, Epoch 230, Loss: 472.3753; Average Loss: 0.5987012655260596; lr: [0.0001]\n",
      "Test Day 15, Epoch 231, Loss: 472.8826; Average Loss: 0.5993442366180614; lr: [0.0001]\n",
      "Test Day 15, Epoch 232, Loss: 472.9605; Average Loss: 0.5994430150369274; lr: [0.0001]\n",
      "Test Day 15, Epoch 233, Loss: 473.4952; Average Loss: 0.6001206000343777; lr: [0.0001]\n",
      "Test Day 15, Epoch 234, Loss: 474.3921; Average Loss: 0.6012573701498324; lr: [0.0001]\n",
      "Test Day 15, Epoch 235, Loss: 471.9087; Average Loss: 0.5981099342665291; lr: [0.0001]\n",
      "Test Day 15, Epoch 236, Loss: 473.8462; Average Loss: 0.6005655598126739; lr: [0.0001]\n",
      "Test Day 15, Epoch 237, Loss: 472.6558; Average Loss: 0.5990568252872907; lr: [0.0001]\n",
      "Test Day 15, Epoch 238, Loss: 472.1023; Average Loss: 0.5983552836042242; lr: [0.0001]\n",
      "Test Day 15, Epoch 239, Loss: 473.9329; Average Loss: 0.6006753786132967; lr: [0.0001]\n",
      "Test Day 15, Epoch 240, Loss: 472.0657; Average Loss: 0.5983088218969386; lr: [0.0001]\n",
      "Test Loss: 0.6429635286331177\n",
      "Test Day 16, Epoch 241, Loss: 472.6001; Average Loss: 0.5982279886173296; lr: [0.0001]\n",
      "Test Day 16, Epoch 242, Loss: 472.8608; Average Loss: 0.5985580299474016; lr: [0.0001]\n",
      "Test Day 16, Epoch 243, Loss: 473.4839; Average Loss: 0.5993467294717136; lr: [0.0001]\n",
      "Test Day 16, Epoch 244, Loss: 472.5746; Average Loss: 0.5981956373287153; lr: [0.0001]\n",
      "Test Day 16, Epoch 245, Loss: 473.2542; Average Loss: 0.5990559964240352; lr: [0.0001]\n",
      "Test Day 16, Epoch 246, Loss: 474.4047; Average Loss: 0.60051230780686; lr: [0.0001]\n",
      "Test Day 16, Epoch 247, Loss: 474.5186; Average Loss: 0.6006564333469053; lr: [0.0001]\n",
      "Test Day 16, Epoch 248, Loss: 474.2252; Average Loss: 0.6002850085874147; lr: [0.0001]\n",
      "Test Day 16, Epoch 249, Loss: 475.5901; Average Loss: 0.6020128238050243; lr: [0.0001]\n",
      "Test Day 16, Epoch 250, Loss: 473.4028; Average Loss: 0.5992441129080857; lr: [0.0001]\n",
      "Test Day 16, Epoch 251, Loss: 474.1214; Average Loss: 0.6001536139959022; lr: [0.0001]\n",
      "Test Day 16, Epoch 252, Loss: 472.3759; Average Loss: 0.5979441389252868; lr: [0.0001]\n",
      "Test Day 16, Epoch 253, Loss: 472.7438; Average Loss: 0.5984098784531219; lr: [0.0001]\n",
      "Test Day 16, Epoch 254, Loss: 474.3775; Average Loss: 0.6004778898214992; lr: [0.0001]\n",
      "Test Day 16, Epoch 255, Loss: 472.9810; Average Loss: 0.5987100915063786; lr: [0.0001]\n",
      "Test Loss: 0.5613002777099609\n",
      "Test Day 17, Epoch 256, Loss: 474.2418; Average Loss: 0.5995471751795406; lr: [0.0001]\n",
      "Test Day 17, Epoch 257, Loss: 473.6556; Average Loss: 0.5988061099528663; lr: [0.0001]\n",
      "Test Day 17, Epoch 258, Loss: 473.6061; Average Loss: 0.5987434845356513; lr: [0.0001]\n",
      "Test Day 17, Epoch 259, Loss: 472.0700; Average Loss: 0.5968015215038198; lr: [0.0001]\n",
      "Test Day 17, Epoch 260, Loss: 474.4242; Average Loss: 0.5997777811043181; lr: [0.0001]\n",
      "Test Day 17, Epoch 261, Loss: 473.6147; Average Loss: 0.5987543643801915; lr: [0.0001]\n",
      "Test Day 17, Epoch 262, Loss: 475.3393; Average Loss: 0.6009346218085018; lr: [0.0001]\n",
      "Test Day 17, Epoch 263, Loss: 474.8831; Average Loss: 0.6003579526725219; lr: [0.0001]\n",
      "Test Day 17, Epoch 264, Loss: 473.8022; Average Loss: 0.5989914326239778; lr: [0.0001]\n",
      "Test Day 17, Epoch 265, Loss: 473.9685; Average Loss: 0.5992015954366073; lr: [0.0001]\n",
      "Test Day 17, Epoch 266, Loss: 473.2377; Average Loss: 0.5982777550910728; lr: [0.0001]\n",
      "Test Day 17, Epoch 267, Loss: 474.8120; Average Loss: 0.6002679636746683; lr: [0.0001]\n",
      "Test Day 17, Epoch 268, Loss: 474.4588; Average Loss: 0.5998214355160102; lr: [0.0001]\n",
      "Test Day 17, Epoch 269, Loss: 473.7352; Average Loss: 0.5989066761754708; lr: [0.0001]\n",
      "Test Day 17, Epoch 270, Loss: 473.8870; Average Loss: 0.5990985648520526; lr: [0.0001]\n",
      "Test Loss: 0.5640311241149902\n",
      "Test Day 18, Epoch 271, Loss: 476.0964; Average Loss: 0.6011318630642362; lr: [0.0001]\n",
      "Test Day 18, Epoch 272, Loss: 474.4376; Average Loss: 0.5990373570509632; lr: [0.0001]\n",
      "Test Day 18, Epoch 273, Loss: 474.6488; Average Loss: 0.5993040330482252; lr: [0.0001]\n",
      "Test Day 18, Epoch 274, Loss: 476.5042; Average Loss: 0.6016467773553097; lr: [0.0001]\n",
      "Test Day 18, Epoch 275, Loss: 475.4289; Average Loss: 0.6002890774697969; lr: [0.0001]\n",
      "Test Day 18, Epoch 276, Loss: 474.5506; Average Loss: 0.5991800252837364; lr: [0.0001]\n",
      "Test Day 18, Epoch 277, Loss: 474.7642; Average Loss: 0.5994497609860969; lr: [0.0001]\n",
      "Test Day 18, Epoch 278, Loss: 474.5152; Average Loss: 0.5991353531076451; lr: [0.0001]\n",
      "Test Day 18, Epoch 279, Loss: 474.7106; Average Loss: 0.5993820994791358; lr: [0.0001]\n",
      "Test Day 18, Epoch 280, Loss: 474.7303; Average Loss: 0.5994070033834438; lr: [0.0001]\n",
      "Test Day 18, Epoch 281, Loss: 474.1257; Average Loss: 0.5986436220130535; lr: [0.0001]\n",
      "Test Day 18, Epoch 282, Loss: 473.9465; Average Loss: 0.5984172953499688; lr: [0.0001]\n",
      "Test Day 18, Epoch 283, Loss: 473.3133; Average Loss: 0.5976178453426169; lr: [0.0001]\n",
      "Test Day 18, Epoch 284, Loss: 474.3719; Average Loss: 0.5989544379590738; lr: [0.0001]\n",
      "Test Day 18, Epoch 285, Loss: 474.7793; Average Loss: 0.5994688103897403; lr: [0.0001]\n",
      "Test Loss: 0.5666072368621826\n",
      "Test Day 19, Epoch 286, Loss: 474.7626; Average Loss: 0.598691774346639; lr: [0.0001]\n",
      "Test Day 19, Epoch 287, Loss: 475.8630; Average Loss: 0.6000794354182625; lr: [0.0001]\n",
      "Test Day 19, Epoch 288, Loss: 474.2990; Average Loss: 0.598107216487466; lr: [0.0001]\n",
      "Test Day 19, Epoch 289, Loss: 474.7626; Average Loss: 0.5986918380852756; lr: [0.0001]\n",
      "Test Day 19, Epoch 290, Loss: 475.8612; Average Loss: 0.6000771684875104; lr: [0.0001]\n",
      "Test Day 19, Epoch 291, Loss: 474.4446; Average Loss: 0.5982908402836188; lr: [0.0001]\n",
      "Test Day 19, Epoch 292, Loss: 476.8939; Average Loss: 0.6013794017469447; lr: [0.0001]\n",
      "Test Day 19, Epoch 293, Loss: 474.7395; Average Loss: 0.5986627311754648; lr: [0.0001]\n",
      "Test Day 19, Epoch 294, Loss: 473.9007; Average Loss: 0.597604891090345; lr: [0.0001]\n",
      "Test Day 19, Epoch 295, Loss: 476.3156; Average Loss: 0.6006501824377765; lr: [0.0001]\n",
      "Test Day 19, Epoch 296, Loss: 475.5270; Average Loss: 0.5996557947666318; lr: [0.0001]\n",
      "Test Day 19, Epoch 297, Loss: 474.8595; Average Loss: 0.5988140009991909; lr: [0.0001]\n",
      "Test Day 19, Epoch 298, Loss: 474.6504; Average Loss: 0.5985502805866179; lr: [0.0001]\n",
      "Test Day 19, Epoch 299, Loss: 474.1205; Average Loss: 0.5978821144729364; lr: [0.0001]\n",
      "Test Day 19, Epoch 300, Loss: 476.2231; Average Loss: 0.6005334902230608; lr: [0.0001]\n",
      "Test Loss: 0.5665556192398071\n",
      "Test Day 20, Epoch 301, Loss: 475.7546; Average Loss: 0.5991871819388056; lr: [0.0001]\n",
      "Test Day 20, Epoch 302, Loss: 475.5276; Average Loss: 0.5989012838310799; lr: [0.0001]\n",
      "Test Day 20, Epoch 303, Loss: 476.3344; Average Loss: 0.5999174370273235; lr: [0.0001]\n",
      "Test Day 20, Epoch 304, Loss: 474.6479; Average Loss: 0.5977932704185659; lr: [0.0001]\n",
      "Test Day 20, Epoch 305, Loss: 475.3973; Average Loss: 0.598737111319823; lr: [0.0001]\n",
      "Test Day 20, Epoch 306, Loss: 476.8764; Average Loss: 0.6005999675625818; lr: [0.0001]\n",
      "Test Day 20, Epoch 307, Loss: 475.0452; Average Loss: 0.5982936455560871; lr: [0.0001]\n",
      "Test Day 20, Epoch 308, Loss: 474.6483; Average Loss: 0.5977937688755448; lr: [0.0001]\n",
      "Test Day 20, Epoch 309, Loss: 476.5938; Average Loss: 0.6002441293346491; lr: [0.0001]\n",
      "Test Day 20, Epoch 310, Loss: 474.7044; Average Loss: 0.5978645209401321; lr: [0.0001]\n",
      "Test Day 20, Epoch 311, Loss: 474.6859; Average Loss: 0.5978411667292904; lr: [0.0001]\n",
      "Test Day 20, Epoch 312, Loss: 475.2607; Average Loss: 0.5985651604774918; lr: [0.0001]\n",
      "Test Day 20, Epoch 313, Loss: 475.2122; Average Loss: 0.5985040088144298; lr: [0.0001]\n",
      "Test Day 20, Epoch 314, Loss: 475.0464; Average Loss: 0.598295226205206; lr: [0.0001]\n",
      "Test Day 20, Epoch 315, Loss: 475.8160; Average Loss: 0.5992645124344141; lr: [0.0001]\n",
      "Test Loss: 0.629747211933136\n",
      "Test Day 21, Epoch 316, Loss: 477.6554; Average Loss: 0.6008243536799209; lr: [0.0001]\n",
      "Test Day 21, Epoch 317, Loss: 476.5818; Average Loss: 0.5994739460495283; lr: [0.0001]\n",
      "Test Day 21, Epoch 318, Loss: 475.3900; Average Loss: 0.5979748935819422; lr: [0.0001]\n",
      "Test Day 21, Epoch 319, Loss: 476.9328; Average Loss: 0.5999154432764593; lr: [0.0001]\n",
      "Test Day 21, Epoch 320, Loss: 475.1930; Average Loss: 0.5977270774121555; lr: [0.0001]\n",
      "Test Day 21, Epoch 321, Loss: 477.0605; Average Loss: 0.6000761403977496; lr: [0.0001]\n",
      "Test Day 21, Epoch 322, Loss: 476.1641; Average Loss: 0.5989485182852116; lr: [0.0001]\n",
      "Test Day 21, Epoch 323, Loss: 475.2940; Average Loss: 0.5978541440184011; lr: [0.0001]\n",
      "Test Day 21, Epoch 324, Loss: 476.1941; Average Loss: 0.5989862549979732; lr: [0.0001]\n"
     ]
    }
   ],
   "source": [
    "true_labels, predicted_scores, companies = agent.train(100, 15) # one column for each company ticker, flatten to compute ROC over all\n",
    "fpr, tpr, thresholds = roc_curve(true_labels.reshape(-1), predicted_scores.reshape(-1))\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_accuracy_prob(gt, pred):\n",
    "    \"\"\"Calculates directional accuracy of given ground truth and \n",
    "    prediction percent change series.\n",
    "    From Kaeley et al.\n",
    "    \n",
    "    inputs:\n",
    "        gt: ground truth pct_change\n",
    "        pred: predicted probability of positive pct_change\n",
    "        \n",
    "    returns:\n",
    "        acc: directional accuracy of predicted values\n",
    "    \"\"\"\n",
    "    pred[pred > 0.5] = 1\n",
    "    pred[pred <= 0.5] = 0\n",
    "    return np.sum(gt == pred) / len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL Directional Accuracy: 56.00%, Percent of days with upward price movement: 56.00%\n"
     ]
    }
   ],
   "source": [
    "gt = np.load(os.path.join('..','saved_models', 'incrementalBCE_AAPL', 'returns', 'ground_truths_05-05_13-27-46.npy'))\n",
    "preds = np.load(os.path.join('..','saved_models', 'incrementalBCE_AAPL', 'returns', 'predictions05-05_13-27-46.npy'))\n",
    "\n",
    "num_increasing = np.sum(gt) / len(gt.ravel())\n",
    "da = directional_accuracy_prob(gt, preds)\n",
    "print(f\"{tickers[0]} Directional Accuracy: {da:.2%}, Percent of days with upward price movement: {num_increasing:.2%}\")\n",
    "\n",
    "# for j in range(gt.shape[1]): # columns correspond to each company\n",
    "#     num_increasing = np.sum(gt) / len(gt.ravel())\n",
    "#     da = directional_accuracy_prob(gt[:,j], preds[:,j])\n",
    "#     print(f\"{tickers[j]} Directional Accuracy: {da:.2%}, Percent of days with upward price movement: {num_increasing:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
