{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from StockDataWrapper import get_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthDataset(Dataset):\n",
    "    def __init__(self, tickers: list[str], data_dir: str, lag_days: int, test_days: int, test: bool):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.tickers = []\n",
    "\n",
    "        num_to_ticker = dict()\n",
    "        ticker_to_num = dict()\n",
    "        ids = np.linspace(-1, 1, len(tickers)).round(5)\n",
    "        for i, num in enumerate(ids):\n",
    "            num_to_ticker[str(num)] = tickers[i]\n",
    "            ticker_to_num[tickers[i]] = num\n",
    "\n",
    "        # get the unix timestamp value for 2024-04-15. Used to scale the date value used as input to the model\n",
    "        cur_unix_timestamp = datetime.datetime(year=2024, month=4, day=15).timestamp()\n",
    "\n",
    "        for ticker in tickers:\n",
    "            ordered_time_series = get_time_series(ticker, data_dir, normalize=True)\n",
    "\n",
    "            if test:\n",
    "                idx = range(len(ordered_time_series) - lag_days - test_days, len(ordered_time_series))\n",
    "            else:\n",
    "                idx = range(lag_days, len(ordered_time_series) - lag_days - test_days)\n",
    "\n",
    "            # need to get lag_days days of input tokens and predict the lag_days+1 day, and so on\n",
    "            for cur_target_idx in idx:\n",
    "\n",
    "                # get target values\n",
    "                cur_target_actual_after = [ticker_to_num[ticker]]\n",
    "                for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                    val = ordered_time_series[cur_target_idx][key]\n",
    "                    if key == 'date':\n",
    "                        val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                    cur_target_actual_after.append(val)\n",
    "\n",
    "                # each \"sample\" (input) will have multiple tokens. Each token is a vector of the day's values\n",
    "                cur_input = []\n",
    "\n",
    "                for i in range(cur_target_idx - lag_days, cur_target_idx):\n",
    "\n",
    "                    cur_token = [ticker_to_num[ticker]]\n",
    "                    for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                        val = ordered_time_series[cur_target_idx][key]\n",
    "                        if key == 'date':\n",
    "                            val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                        cur_token.append(val)\n",
    "\n",
    "                    cur_input.append(cur_token)\n",
    "\n",
    "                self.X.append(cur_input)\n",
    "                self.y.append(cur_target_actual_after)\n",
    "                self.tickers.append(ticker)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32), self.tickers[idx]\n",
    "    \n",
    "\n",
    "class IncrementalDataset(Dataset):\n",
    "    def __init__(self, tickers: list[str], data_dir: str, lag_days: int, test_days: int, test: bool):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.tickers = []\n",
    "\n",
    "        num_to_ticker = dict()\n",
    "        ticker_to_num = dict()\n",
    "        ids = np.linspace(-1, 1, len(tickers)).round(5)\n",
    "        for i, num in enumerate(ids):\n",
    "            num_to_ticker[str(num)] = tickers[i]\n",
    "            ticker_to_num[tickers[i]] = num\n",
    "\n",
    "        # get the unix timestamp value for 2024-04-15. Used to scale the date value used as input to the model\n",
    "        cur_unix_timestamp = datetime.datetime(year=2024, month=4, day=15).timestamp()\n",
    "\n",
    "        for ticker in tickers:\n",
    "            ordered_time_series = get_time_series(ticker, data_dir, normalize=True)\n",
    "\n",
    "            if test:\n",
    "                idx = range(len(ordered_time_series) - lag_days - test_days, len(ordered_time_series) - lag_days - test_days + 1)\n",
    "            else:\n",
    "                idx = range(lag_days + 1500, len(ordered_time_series) - lag_days - test_days) # CUT OFF OLD PART OF DATA!!!!!!!!!!!!! FYI\n",
    "\n",
    "            # need to get lag_days days of input tokens and predict the lag_days+1 day, and so on\n",
    "            for cur_target_idx in idx:\n",
    "\n",
    "                # get target values\n",
    "                cur_target_actual_after = [ticker_to_num[ticker]]\n",
    "                for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                    val = ordered_time_series[cur_target_idx][key]\n",
    "                    if key == 'date':\n",
    "                        val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                    cur_target_actual_after.append(val)\n",
    "\n",
    "                # each \"sample\" (input) will have multiple tokens. Each token is a vector of the day's values\n",
    "                cur_input = []\n",
    "\n",
    "                for i in range(cur_target_idx - lag_days, cur_target_idx):\n",
    "\n",
    "                    cur_token = [ticker_to_num[ticker]]\n",
    "                    for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                        val = ordered_time_series[cur_target_idx][key]\n",
    "                        if key == 'date':\n",
    "                            val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                        cur_token.append(val)\n",
    "\n",
    "                    cur_input.append(cur_token)\n",
    "\n",
    "                self.X.append(cur_input)\n",
    "                self.y.append(cur_target_actual_after)\n",
    "                self.tickers.append(ticker)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32), self.tickers[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer (batch first)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class BigTransformer(nn.Module):\n",
    "    def __init__(self, indim, outdim, hidden_dim=256, d_model=64, nhead=4, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super(BigTransformer, self).__init__()\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(indim, hidden_dim),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "            )\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, batch_first=True)\n",
    "\n",
    "        self.decoder_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, outdim)\n",
    "        )\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "    # src: (B, S, F) batches, sequence length, features\n",
    "    # tgt: (B, F)\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.embedding(tgt.unsqueeze(1))\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src.shape[1]).to(device)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.shape[1]).to(device)\n",
    "        out = self.transformer(src, tgt, src_mask, tgt_mask, src_is_causal=True, tgt_is_causal=True)\n",
    "        out = self.decoder_mlp(out)\n",
    "        return out.squeeze(1)\n",
    "    \n",
    "    def generate(self, src, max_len=100, start_token=None, end_token=None):\n",
    "        device = src.device\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src.shape[1]).to(device)\n",
    "\n",
    "        output_seq = torch.zeros((src.shape[0], 1, self.d_model)).to(device)\n",
    "        if start_token is not None:\n",
    "            output_seq[:, 0] = self.embedding(start_token.to(device))\n",
    "\n",
    "        for i in range(max_len):\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(output_seq.shape[1]).to(device)\n",
    "            next_token = self.transformer(src, output_seq, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "            output_seq = torch.cat([output_seq, next_token], dim=1)\n",
    "\n",
    "            if end_token is not None and torch.all(next_token == end_token):\n",
    "                break\n",
    "            \n",
    "        output_seq = self.decoder_mlp(output_seq[:, 1:]) # exclude start token\n",
    "        \n",
    "        return output_seq.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# big transformer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public methods are constructor, train, evaluate, and load_model\n",
    "class BigTransformerAgent:\n",
    "    def __init__(self, indim, outdim, hidden_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 device, checkpoint_dir, init_lr, lr_decay, min_lr, decay_lr_every):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        self.allow_training = True\n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.device = device\n",
    "        self.model = BigTransformer(indim, outdim, hidden_dim, d_model, nhead, \n",
    "                                    num_encoder_layers, num_decoder_layers)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=init_lr)\n",
    "\n",
    "        # at epoch e, lr will be init_lr * scheduler_lamb(e)\n",
    "        scheduler_lamb = lambda epoch: max(lr_decay ** (epoch // decay_lr_every), min_lr / init_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, scheduler_lamb)\n",
    "\n",
    "        self.criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cur_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        self.allow_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def __save_model(self):\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "\n",
    "        dt = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(self.checkpoint_dir, f\"{dt}_agent_{self.cur_epoch}.pth\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"epoch\": self.cur_epoch\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def __save_returns(self, returns):\n",
    "        returns_prefix = f\"{self.checkpoint_dir}/returns\"\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        if not os.path.isdir(returns_prefix):\n",
    "            os.mkdir(returns_prefix)\n",
    "\n",
    "        np_filename = f\"{returns_prefix}/{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.npy\"\n",
    "        np.save(np_filename, returns)\n",
    "\n",
    "        return np_filename\n",
    "\n",
    "    def train(self, dataloader, epochs):\n",
    "        if not self.allow_training:\n",
    "            raise Exception(\"Not allowed to train after loading\")\n",
    "\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for (X, y, tickers) in dataloader:\n",
    "                X = X.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                y_pred = self.model(X, y)\n",
    "\n",
    "                loss = self.criterion(y, y_pred)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # TODO is this good?\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # self.scheduler.step()\n",
    "\n",
    "            losses.append(epoch_loss)\n",
    "\n",
    "            self.cur_epoch += 1\n",
    "\n",
    "            if self.cur_epoch % 25 == 0:\n",
    "                self.__save_model()\n",
    "\n",
    "            average_loss = epoch_loss / len(dataloader.dataset)  # epoch loss is the sum of each sample's loss since mse reduction is sum\n",
    "            print(f\"Epoch {self.cur_epoch}, Loss: {epoch_loss:.4f}; Average Loss: {average_loss}; lr: {self.scheduler.get_last_lr()}\")\n",
    "\n",
    "        self.model.eval()\n",
    "        return self.__save_returns(losses)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        gt = torch.Tensor([])\n",
    "        preds = torch.Tensor([])\n",
    "        companies = []\n",
    "        for batch, (X, y, co) in enumerate(dataloader):\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            y_pred = self.model.generate(X, max_len=1)\n",
    "\n",
    "            loss = self.criterion(y, y_pred)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            gt = torch.concat([gt, y], dim=0)\n",
    "            preds = torch.concat([preds, y_pred], dim=0)\n",
    "            companies.extend(co)\n",
    "\n",
    "        print(f\"Average Loss: {total_loss / len(dataloader.dataset)}\")  # total loss is the sum of each sample's loss since mse reduction is sum\n",
    "\n",
    "        gt = gt.to('cpu')\n",
    "        preds = preds.to('cpu')\n",
    "\n",
    "        return gt, preds, companies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# incremental transformer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public methods are constructor, train, evaluate, and load_model\n",
    "# agent for incremental learning models\n",
    "class IncrementalAgent:\n",
    "    def __init__(self, indim, outdim, hidden_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 device, checkpoint_dir, init_lr, lr_decay, min_lr, decay_lr_every, tickers):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.tickers = tickers\n",
    "\n",
    "        self.allow_training = True\n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.device = device\n",
    "        self.model = BigTransformer(indim, outdim, hidden_dim, d_model, nhead, \n",
    "                                    num_encoder_layers, num_decoder_layers)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=init_lr)\n",
    "\n",
    "        # at epoch e, lr will be init_lr * scheduler_lamb(e)\n",
    "        scheduler_lamb = lambda epoch: max(lr_decay ** (epoch // decay_lr_every), min_lr / init_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, scheduler_lamb)\n",
    "\n",
    "        self.criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cur_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        self.allow_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def __save_model(self):\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "\n",
    "        dt = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(self.checkpoint_dir, f\"{dt}_agent_{self.cur_epoch}.pth\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"epoch\": self.cur_epoch\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def __save_returns(self, fn, returns):\n",
    "        returns_prefix = f\"{self.checkpoint_dir}/returns\"\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        if not os.path.isdir(returns_prefix):\n",
    "            os.mkdir(returns_prefix)\n",
    "\n",
    "        np_filename = f\"{returns_prefix}/{fn}{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.npy\"\n",
    "        np.save(np_filename, returns)\n",
    "\n",
    "        return np_filename\n",
    "\n",
    "\n",
    "    def train(self, test_days, epochs):\n",
    "        if not self.allow_training:\n",
    "            raise Exception(\"Not allowed to train after loading\")\n",
    "        \n",
    "\n",
    "        losses = []\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        companies = []\n",
    "\n",
    "        for day in range(test_days):\n",
    "            # create new datasets for each new test_day\n",
    "            train_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, False)\n",
    "            test_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, True)\n",
    "            train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "            test_loader = DataLoader(test_set)\n",
    "\n",
    "            losses_per_day = []\n",
    "            self.model.train()\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for (X, y, tickers) in train_loader:\n",
    "                    X = X.to(self.device)\n",
    "                    y = y.to(self.device)\n",
    "\n",
    "                    y_pred = self.model(X, y)\n",
    "\n",
    "                    loss = self.criterion(y_pred, y)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # TODO is this good?\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                # self.scheduler.step()\n",
    "\n",
    "                losses_per_day.append(epoch_loss)\n",
    "\n",
    "                self.cur_epoch += 1\n",
    "\n",
    "                if self.cur_epoch % 25 == 0:\n",
    "                    self.__save_model()\n",
    "\n",
    "                average_loss = epoch_loss / len(train_set)  # epoch loss is the sum of each sample's loss since mse reduction is sum\n",
    "                print(f\"Test Day {day}, Epoch {self.cur_epoch}, Loss: {epoch_loss:.4f}; Average Loss: {average_loss}; lr: {self.scheduler.get_last_lr()}\")\n",
    "\n",
    "            gt, pred, tickers = self.evaluate(test_loader)\n",
    "            ground_truths.append(gt)\n",
    "            predictions.append(pred)\n",
    "            companies.extend(tickers)\n",
    "\n",
    "            losses.append(losses_per_day)\n",
    "\n",
    "        ground_truths = np.array(ground_truths)\n",
    "        predictions = np.array(ground_truths)\n",
    "\n",
    "        self.__save_returns('ground_truths_', ground_truths)\n",
    "        self.__save_returns('predictions', predictions)\n",
    "\n",
    "        self.model.eval()\n",
    "        return ground_truths, predictions, companies\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        gt = torch.Tensor([])\n",
    "        preds = torch.Tensor([])\n",
    "        companies = []\n",
    "        for batch, (X, y, co) in enumerate(dataloader):\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            y_pred = self.model.generate(X, max_len=1)\n",
    "\n",
    "            loss = self.criterion(y_pred, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            gt = torch.concat([gt, y], dim=0)\n",
    "            preds = torch.concat([preds, y_pred], dim=0)\n",
    "            companies.extend(co)\n",
    "\n",
    "        print(f\"Test Loss: {total_loss / len(dataloader.dataset)}\")  # total loss is the sum of each sample's loss since mse reduction is sum\n",
    "\n",
    "        gt = gt.to('cpu').detach().numpy()\n",
    "        preds = preds.to('cpu').detach().numpy()\n",
    "\n",
    "        return gt, preds, companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Probability Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public methods are constructor, train, evaluate, and load_model\n",
    "# agent for incremental learning models\n",
    "class IncrementalProbAgent:\n",
    "    def __init__(self, indim, outdim, hidden_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 device, checkpoint_dir, init_lr, lr_decay, min_lr, decay_lr_every, tickers):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.tickers = tickers\n",
    "\n",
    "        self.allow_training = True\n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.device = device\n",
    "        self.model = BigTransformer(indim, outdim, hidden_dim, d_model, nhead, \n",
    "                                    num_encoder_layers, num_decoder_layers)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=init_lr)\n",
    "\n",
    "        # at epoch e, lr will be init_lr * scheduler_lamb(e)\n",
    "        scheduler_lamb = lambda epoch: max(lr_decay ** (epoch // decay_lr_every), min_lr / init_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, scheduler_lamb)\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cur_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        self.allow_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def __save_model(self):\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "\n",
    "        dt = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(self.checkpoint_dir, f\"{dt}_agent_{self.cur_epoch}.pth\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"epoch\": self.cur_epoch\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def __save_returns(self, fn, returns):\n",
    "        returns_prefix = f\"{self.checkpoint_dir}/returns\"\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        if not os.path.isdir(returns_prefix):\n",
    "            os.mkdir(returns_prefix)\n",
    "\n",
    "        np_filename = f\"{returns_prefix}/{fn}{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.npy\"\n",
    "        np.save(np_filename, returns)\n",
    "\n",
    "        return np_filename\n",
    "\n",
    "    def train(self, test_days, epochs):\n",
    "        if not self.allow_training:\n",
    "            raise Exception(\"Not allowed to train after loading\")\n",
    "        \n",
    "        losses = []\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        companies = []\n",
    "\n",
    "        for day in range(test_days):\n",
    "            # create new datasets for each new test_day\n",
    "            train_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, False)\n",
    "            test_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, True)\n",
    "            train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "            test_loader = DataLoader(test_set)\n",
    "\n",
    "            losses_per_day = []\n",
    "            self.model.train()\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for (X, y, tickers) in train_loader:\n",
    "                    X = X.to(self.device)\n",
    "                    y = y.to(self.device)\n",
    "\n",
    "                    y_pred = self.model(X, y) # y_pred will be (B, 1)\n",
    "\n",
    "                    y = y[:,7].clone() # pct_change is 7th column\n",
    "                    y[y <= 0] = 0\n",
    "                    y[y > 0] = 1\n",
    "\n",
    "                    loss = self.criterion(y_pred, y.unsqueeze(1))\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # TODO is this good?\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                # self.scheduler.step()\n",
    "\n",
    "                losses_per_day.append(epoch_loss)\n",
    "\n",
    "                self.cur_epoch += 1\n",
    "\n",
    "                if self.cur_epoch % 25 == 0:\n",
    "                    self.__save_model()\n",
    "\n",
    "                average_loss = epoch_loss / len(train_set)  # epoch loss is the sum of each sample's loss since mse reduction is sum\n",
    "                print(f\"Test Day {day}, Epoch {self.cur_epoch}, Loss: {epoch_loss:.4f}; Average Loss: {average_loss}; lr: {self.scheduler.get_last_lr()}\")\n",
    "\n",
    "            gt, pred, tickers = self.evaluate(test_loader)\n",
    "            ground_truths.append(gt)\n",
    "            predictions.append(pred)\n",
    "            companies.extend(tickers)\n",
    "\n",
    "            losses.append(losses_per_day)\n",
    "\n",
    "        ground_truths = np.array(ground_truths).squeeze()\n",
    "        predictions = np.array(predictions).squeeze()\n",
    "\n",
    "        self.__save_returns('ground_truths_', ground_truths)\n",
    "        self.__save_returns('predictions', predictions)\n",
    "\n",
    "        self.model.eval()\n",
    "        return ground_truths, predictions, companies\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        gt = torch.Tensor([]).to(self.device)\n",
    "        preds = torch.Tensor([]).to(self.device)\n",
    "        companies = []\n",
    "        for batch, (X, y, co) in enumerate(dataloader):\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            y_pred = self.model.generate(X, max_len=1)\n",
    "\n",
    "            y = y[:,7].clone() # pct_change is 7th column\n",
    "            y[y <= 0] = 0\n",
    "            y[y > 0] = 1\n",
    "\n",
    "            loss = self.criterion(y_pred, y.unsqueeze(1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            gt = torch.concat([gt, y.unsqueeze(1)], dim=0)\n",
    "            preds = torch.concat([preds, torch.sigmoid(y_pred)], dim=0)\n",
    "            companies.extend(co)\n",
    "\n",
    "        print(f\"Test Loss: {total_loss / len(dataloader.dataset)}\")  # total loss is the sum of each sample's loss since mse reduction is sum\n",
    "\n",
    "        gt = gt.to('cpu').detach().numpy()\n",
    "        preds = preds.to('cpu').detach().numpy()\n",
    "\n",
    "        return gt, preds, companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "tickers = [\"AAPL\", \"NVDA\", 'MSFT', 'TSLA', 'AMZN']\n",
    "data_dir = \"../data_combined/\"\n",
    "test_days = 100\n",
    "lag_days = 30\n",
    "batch_size = 128\n",
    "\n",
    "# model params\n",
    "indim = 23\n",
    "outdim = 1\n",
    "hidden_dim = 256\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = VariableLengthDataset(tickers=tickers, data_dir=data_dir, lag_days=lag_days, test_days=test_days, test=False)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = VariableLengthDataset(tickers=tickers, data_dir=data_dir, lag_days=lag_days, test_days=test_days, test=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BigTransformerAgent(indim=23, outdim=23, hidden_dim=256, d_model=64, nhead=8, num_encoder_layers=6, num_decoder_layers=6, device=device, \n",
    "                            checkpoint_dir='../saved_models/23outdimMSEmodel', init_lr=0.001, lr_decay=0.0001, min_lr=0.0000025, decay_lr_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(train_loader, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = IncrementalAgent(indim=23, outdim=23, hidden_dim=256, d_model=64, nhead=8, num_encoder_layers=6, num_decoder_layers=6, device=device, \n",
    "                            checkpoint_dir='../saved_models/23outdimIncrementalMSEmodel', init_lr=0.0001, lr_decay=0.5, min_lr=0.00000025, decay_lr_every=10, \n",
    "                            tickers=tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, preds, companies = agent.train(test_days, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = IncrementalProbAgent(indim=23, outdim=1, hidden_dim=256, d_model=64, nhead=8, num_encoder_layers=6, num_decoder_layers=6, device=device, \n",
    "                            checkpoint_dir='../saved_models/1outdimIncrementalBCEmodel', init_lr=0.0001, lr_decay=0.5, min_lr=0.00000025, decay_lr_every=10, \n",
    "                            tickers=tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Day 0, Epoch 1, Loss: 2929.2925; Average Loss: 0.6941451312241396; lr: [0.0001]\n",
      "Test Day 0, Epoch 2, Loss: 2928.0696; Average Loss: 0.6938553633848072; lr: [0.0001]\n",
      "Test Day 0, Epoch 3, Loss: 2927.9721; Average Loss: 0.693832260963476; lr: [0.0001]\n",
      "Test Day 0, Epoch 4, Loss: 2926.4582; Average Loss: 0.6934735004370811; lr: [0.0001]\n",
      "Test Day 0, Epoch 5, Loss: 2927.9585; Average Loss: 0.6938290392618044; lr: [0.0001]\n",
      "Test Day 0, Epoch 6, Loss: 2924.5687; Average Loss: 0.6930257661647706; lr: [0.0001]\n",
      "Test Day 0, Epoch 7, Loss: 2923.9227; Average Loss: 0.6928726720583948; lr: [0.0001]\n",
      "Test Day 0, Epoch 8, Loss: 2924.9852; Average Loss: 0.69312446015706; lr: [0.0001]\n",
      "Test Day 0, Epoch 9, Loss: 2926.9262; Average Loss: 0.6935844077883173; lr: [0.0001]\n",
      "Test Day 0, Epoch 10, Loss: 2924.4415; Average Loss: 0.6929956092653682; lr: [0.0001]\n",
      "Test Day 0, Epoch 11, Loss: 2925.2151; Average Loss: 0.6931789362035091; lr: [0.0001]\n",
      "Test Day 0, Epoch 12, Loss: 2926.5867; Average Loss: 0.6935039592580208; lr: [0.0001]\n",
      "Test Day 0, Epoch 13, Loss: 2930.8511; Average Loss: 0.6945144843151219; lr: [0.0001]\n",
      "Test Day 0, Epoch 14, Loss: 2921.9910; Average Loss: 0.6924149201379568; lr: [0.0001]\n",
      "Test Day 0, Epoch 15, Loss: 2927.7928; Average Loss: 0.6937897523997519; lr: [0.0001]\n",
      "Test Loss: 0.6847132563591003\n",
      "Test Day 1, Epoch 16, Loss: 2933.8146; Average Loss: 0.6943939813495388; lr: [0.0001]\n",
      "Test Day 1, Epoch 17, Loss: 2927.7361; Average Loss: 0.692955289493651; lr: [0.0001]\n",
      "Test Day 1, Epoch 18, Loss: 2926.1703; Average Loss: 0.6925846910617761; lr: [0.0001]\n",
      "Test Day 1, Epoch 19, Loss: 2929.9987; Average Loss: 0.693490825954979; lr: [0.0001]\n",
      "Test Day 1, Epoch 20, Loss: 2926.6191; Average Loss: 0.692690907864881; lr: [0.0001]\n",
      "Test Day 1, Epoch 21, Loss: 2928.3300; Average Loss: 0.6930958537801483; lr: [0.0001]\n",
      "Test Day 1, Epoch 22, Loss: 2931.6011; Average Loss: 0.6938700789107374; lr: [0.0001]\n",
      "Test Day 1, Epoch 23, Loss: 2927.2591; Average Loss: 0.6928423981271552; lr: [0.0001]\n",
      "Test Day 1, Epoch 24, Loss: 2925.3635; Average Loss: 0.6923937175965168; lr: [0.0001]\n",
      "Test Day 1, Epoch 25, Loss: 2924.9109; Average Loss: 0.6922866018989382; lr: [0.0001]\n",
      "Test Day 1, Epoch 26, Loss: 2929.1774; Average Loss: 0.6932964338071248; lr: [0.0001]\n",
      "Test Day 1, Epoch 27, Loss: 2926.1946; Average Loss: 0.6925904297123294; lr: [0.0001]\n",
      "Test Day 1, Epoch 28, Loss: 2926.8331; Average Loss: 0.6927415509336798; lr: [0.0001]\n",
      "Test Day 1, Epoch 29, Loss: 2926.4394; Average Loss: 0.692648377531379; lr: [0.0001]\n",
      "Test Day 1, Epoch 30, Loss: 2926.1096; Average Loss: 0.6925703118679791; lr: [0.0001]\n",
      "Test Loss: 0.6804172039031983\n",
      "Test Day 2, Epoch 31, Loss: 2930.9768; Average Loss: 0.6929023129438395; lr: [0.0001]\n",
      "Test Day 2, Epoch 32, Loss: 2928.4868; Average Loss: 0.6923136732538823; lr: [0.0001]\n",
      "Test Day 2, Epoch 33, Loss: 2928.4148; Average Loss: 0.6922966524218837; lr: [0.0001]\n",
      "Test Day 2, Epoch 34, Loss: 2930.5056; Average Loss: 0.692790927390962; lr: [0.0001]\n",
      "Test Day 2, Epoch 35, Loss: 2930.0953; Average Loss: 0.6926939310475162; lr: [0.0001]\n",
      "Test Day 2, Epoch 36, Loss: 2928.4761; Average Loss: 0.6923111407751169; lr: [0.0001]\n",
      "Test Day 2, Epoch 37, Loss: 2930.1184; Average Loss: 0.6926993753329518; lr: [0.0001]\n",
      "Test Day 2, Epoch 38, Loss: 2930.1443; Average Loss: 0.6927055031977083; lr: [0.0001]\n",
      "Test Day 2, Epoch 39, Loss: 2932.2668; Average Loss: 0.6932072929174906; lr: [0.0001]\n",
      "Test Day 2, Epoch 40, Loss: 2930.7238; Average Loss: 0.6928425076441844; lr: [0.0001]\n",
      "Test Day 2, Epoch 41, Loss: 2929.8374; Average Loss: 0.6926329626259229; lr: [0.0001]\n",
      "Test Day 2, Epoch 42, Loss: 2928.7623; Average Loss: 0.6923787870023831; lr: [0.0001]\n",
      "Test Day 2, Epoch 43, Loss: 2930.0661; Average Loss: 0.6926870206287284; lr: [0.0001]\n",
      "Test Day 2, Epoch 44, Loss: 2933.2577; Average Loss: 0.6934415476823811; lr: [0.0001]\n",
      "Test Day 2, Epoch 45, Loss: 2930.0814; Average Loss: 0.6926906281329216; lr: [0.0001]\n",
      "Test Loss: 0.6610267877578735\n",
      "Test Day 3, Epoch 46, Loss: 2933.2983; Average Loss: 0.6926324139519594; lr: [0.0001]\n",
      "Test Day 3, Epoch 47, Loss: 2934.4309; Average Loss: 0.6928998485506637; lr: [0.0001]\n",
      "Test Day 3, Epoch 48, Loss: 2934.4265; Average Loss: 0.6928988287834519; lr: [0.0001]\n",
      "Test Day 3, Epoch 49, Loss: 2934.2445; Average Loss: 0.6928558373254192; lr: [0.0001]\n",
      "Test Day 3, Epoch 50, Loss: 2932.6182; Average Loss: 0.6924718301721278; lr: [0.0001]\n",
      "Test Day 3, Epoch 51, Loss: 2932.3351; Average Loss: 0.6924049915642496; lr: [0.0001]\n",
      "Test Day 3, Epoch 52, Loss: 2934.6801; Average Loss: 0.6929587113111332; lr: [0.0001]\n",
      "Test Day 3, Epoch 53, Loss: 2934.3670; Average Loss: 0.6928847745289628; lr: [0.0001]\n",
      "Test Day 3, Epoch 54, Loss: 2933.9108; Average Loss: 0.6927770393939058; lr: [0.0001]\n",
      "Test Day 3, Epoch 55, Loss: 2928.6298; Average Loss: 0.6915300625977859; lr: [0.0001]\n",
      "Test Day 3, Epoch 56, Loss: 2931.1428; Average Loss: 0.6921234500168909; lr: [0.0001]\n",
      "Test Day 3, Epoch 57, Loss: 2934.2733; Average Loss: 0.6928626558152956; lr: [0.0001]\n",
      "Test Day 3, Epoch 58, Loss: 2935.0494; Average Loss: 0.6930458916683264; lr: [0.0001]\n",
      "Test Day 3, Epoch 59, Loss: 2932.7019; Average Loss: 0.6924915878944442; lr: [0.0001]\n",
      "Test Day 3, Epoch 60, Loss: 2932.4235; Average Loss: 0.69242586520937; lr: [0.0001]\n",
      "Test Loss: 0.6908831834793091\n",
      "Test Day 4, Epoch 61, Loss: 2936.2730; Average Loss: 0.6925172149010425; lr: [0.0001]\n",
      "Test Day 4, Epoch 62, Loss: 2937.4936; Average Loss: 0.6928050904903772; lr: [0.0001]\n",
      "Test Day 4, Epoch 63, Loss: 2937.3327; Average Loss: 0.6927671454987436; lr: [0.0001]\n",
      "Test Day 4, Epoch 64, Loss: 2938.1816; Average Loss: 0.6929673516525412; lr: [0.0001]\n",
      "Test Day 4, Epoch 65, Loss: 2935.6743; Average Loss: 0.6923760222938825; lr: [0.0001]\n",
      "Test Day 4, Epoch 66, Loss: 2933.3489; Average Loss: 0.6918275824132956; lr: [0.0001]\n",
      "Test Day 4, Epoch 67, Loss: 2936.3400; Average Loss: 0.6925330175543731; lr: [0.0001]\n",
      "Test Day 4, Epoch 68, Loss: 2933.7427; Average Loss: 0.6919204392523136; lr: [0.0001]\n",
      "Test Day 4, Epoch 69, Loss: 2934.7647; Average Loss: 0.6921614817853244; lr: [0.0001]\n",
      "Test Day 4, Epoch 70, Loss: 2936.8828; Average Loss: 0.6926610260639551; lr: [0.0001]\n",
      "Test Day 4, Epoch 71, Loss: 2931.4184; Average Loss: 0.6913722587081621; lr: [0.0001]\n",
      "Test Day 4, Epoch 72, Loss: 2934.6528; Average Loss: 0.6921351025689323; lr: [0.0001]\n",
      "Test Day 4, Epoch 73, Loss: 2933.5561; Average Loss: 0.6918764343801534; lr: [0.0001]\n",
      "Test Day 4, Epoch 74, Loss: 2933.6741; Average Loss: 0.6919042731231113; lr: [0.0001]\n",
      "Test Day 4, Epoch 75, Loss: 2932.2308; Average Loss: 0.6915638745955701; lr: [0.0001]\n",
      "Test Loss: 0.7251539111137391\n",
      "Test Day 5, Epoch 76, Loss: 2941.7089; Average Loss: 0.6929820816705028; lr: [0.0001]\n",
      "Test Day 5, Epoch 77, Loss: 2942.1476; Average Loss: 0.6930854123388499; lr: [0.0001]\n",
      "Test Day 5, Epoch 78, Loss: 2941.7372; Average Loss: 0.6929887412153228; lr: [0.0001]\n",
      "Test Day 5, Epoch 79, Loss: 2937.0296; Average Loss: 0.6918797601940213; lr: [0.0001]\n",
      "Test Day 5, Epoch 80, Loss: 2938.7995; Average Loss: 0.6922967110983194; lr: [0.0001]\n",
      "Test Day 5, Epoch 81, Loss: 2940.1639; Average Loss: 0.6926181240553569; lr: [0.0001]\n",
      "Test Day 5, Epoch 82, Loss: 2939.0905; Average Loss: 0.6923652446733347; lr: [0.0001]\n",
      "Test Day 5, Epoch 83, Loss: 2939.4327; Average Loss: 0.6924458581230244; lr: [0.0001]\n",
      "Test Day 5, Epoch 84, Loss: 2940.2680; Average Loss: 0.692642648425063; lr: [0.0001]\n",
      "Test Day 5, Epoch 85, Loss: 2941.3376; Average Loss: 0.6928946051356087; lr: [0.0001]\n",
      "Test Day 5, Epoch 86, Loss: 2938.0088; Average Loss: 0.692110441991943; lr: [0.0001]\n",
      "Test Day 5, Epoch 87, Loss: 2940.4007; Average Loss: 0.6926738963671932; lr: [0.0001]\n",
      "Test Day 5, Epoch 88, Loss: 2939.6173; Average Loss: 0.6924893611732725; lr: [0.0001]\n",
      "Test Day 5, Epoch 89, Loss: 2942.2340; Average Loss: 0.693105764130681; lr: [0.0001]\n",
      "Test Day 5, Epoch 90, Loss: 2937.9491; Average Loss: 0.6920963763629028; lr: [0.0001]\n",
      "Test Loss: 0.6476833462715149\n",
      "Test Day 6, Epoch 91, Loss: 2940.1288; Average Loss: 0.6917950107350069; lr: [0.0001]\n",
      "Test Day 6, Epoch 92, Loss: 2942.7044; Average Loss: 0.6924010413674747; lr: [0.0001]\n",
      "Test Day 6, Epoch 93, Loss: 2941.3115; Average Loss: 0.6920733027738684; lr: [0.0001]\n",
      "Test Day 6, Epoch 94, Loss: 2941.3488; Average Loss: 0.6920820693969727; lr: [0.0001]\n",
      "Test Day 6, Epoch 95, Loss: 2941.6972; Average Loss: 0.692164052177878; lr: [0.0001]\n",
      "Test Day 6, Epoch 96, Loss: 2939.1480; Average Loss: 0.6915642444386202; lr: [0.0001]\n",
      "Test Day 6, Epoch 97, Loss: 2947.6932; Average Loss: 0.6935748811609604; lr: [0.0001]\n",
      "Test Day 6, Epoch 98, Loss: 2940.4351; Average Loss: 0.6918670717127183; lr: [0.0001]\n",
      "Test Day 6, Epoch 99, Loss: 2945.1044; Average Loss: 0.6929657395306755; lr: [0.0001]\n",
      "Test Day 6, Epoch 100, Loss: 2944.3308; Average Loss: 0.6927837066650391; lr: [0.0001]\n",
      "Test Day 6, Epoch 101, Loss: 2940.8554; Average Loss: 0.6919659733491785; lr: [0.0001]\n",
      "Test Day 6, Epoch 102, Loss: 2944.3489; Average Loss: 0.6927879773308249; lr: [0.0001]\n",
      "Test Day 6, Epoch 103, Loss: 2939.3139; Average Loss: 0.6916032741771024; lr: [0.0001]\n",
      "Test Day 6, Epoch 104, Loss: 2943.1713; Average Loss: 0.6925108934290268; lr: [0.0001]\n",
      "Test Day 6, Epoch 105, Loss: 2943.3940; Average Loss: 0.6925632826861213; lr: [0.0001]\n",
      "Test Loss: 0.7063952922821045\n",
      "Test Day 7, Epoch 106, Loss: 2945.4667; Average Loss: 0.6922365984261106; lr: [0.0001]\n",
      "Test Day 7, Epoch 107, Loss: 2946.3148; Average Loss: 0.6924359062723772; lr: [0.0001]\n",
      "Test Day 7, Epoch 108, Loss: 2948.5722; Average Loss: 0.692966436975571; lr: [0.0001]\n",
      "Test Day 7, Epoch 109, Loss: 2949.0560; Average Loss: 0.6930801458840645; lr: [0.0001]\n",
      "Test Day 7, Epoch 110, Loss: 2944.5127; Average Loss: 0.6920123852237832; lr: [0.0001]\n",
      "Test Day 7, Epoch 111, Loss: 2943.7754; Average Loss: 0.6918391110053774; lr: [0.0001]\n",
      "Test Day 7, Epoch 112, Loss: 2944.6195; Average Loss: 0.6920374967796962; lr: [0.0001]\n",
      "Test Day 7, Epoch 113, Loss: 2944.8764; Average Loss: 0.6920978528212156; lr: [0.0001]\n",
      "Test Day 7, Epoch 114, Loss: 2946.8042; Average Loss: 0.6925509227568899; lr: [0.0001]\n",
      "Test Day 7, Epoch 115, Loss: 2942.0651; Average Loss: 0.691437157564802; lr: [0.0001]\n",
      "Test Day 7, Epoch 116, Loss: 2943.4590; Average Loss: 0.6917647365117325; lr: [0.0001]\n",
      "Test Day 7, Epoch 117, Loss: 2945.0191; Average Loss: 0.6921313934684781; lr: [0.0001]\n",
      "Test Day 7, Epoch 118, Loss: 2946.3901; Average Loss: 0.6924536053918644; lr: [0.0001]\n",
      "Test Day 7, Epoch 119, Loss: 2945.1479; Average Loss: 0.6921616676411253; lr: [0.0001]\n",
      "Test Day 7, Epoch 120, Loss: 2945.2591; Average Loss: 0.6921877958519619; lr: [0.0001]\n",
      "Test Loss: 0.6445635437965394\n",
      "Test Day 8, Epoch 121, Loss: 2948.7204; Average Loss: 0.6921878868425396; lr: [0.0001]\n",
      "Test Day 8, Epoch 122, Loss: 2949.3513; Average Loss: 0.6923359745545007; lr: [0.0001]\n",
      "Test Day 8, Epoch 123, Loss: 2946.9387; Average Loss: 0.6917696402106487; lr: [0.0001]\n",
      "Test Day 8, Epoch 124, Loss: 2948.1114; Average Loss: 0.6920449288238382; lr: [0.0001]\n",
      "Test Day 8, Epoch 125, Loss: 2947.6752; Average Loss: 0.6919425413642131; lr: [0.0001]\n",
      "Test Day 8, Epoch 126, Loss: 2947.4258; Average Loss: 0.6918839960590775; lr: [0.0001]\n",
      "Test Day 8, Epoch 127, Loss: 2949.1711; Average Loss: 0.692293680217904; lr: [0.0001]\n",
      "Test Day 8, Epoch 128, Loss: 2949.0489; Average Loss: 0.6922650099919995; lr: [0.0001]\n",
      "Test Day 8, Epoch 129, Loss: 2945.1882; Average Loss: 0.6913587355277908; lr: [0.0001]\n",
      "Test Day 8, Epoch 130, Loss: 2946.8662; Average Loss: 0.6917526218253123; lr: [0.0001]\n",
      "Test Day 8, Epoch 131, Loss: 2949.0200; Average Loss: 0.6922582124880222; lr: [0.0001]\n",
      "Test Day 8, Epoch 132, Loss: 2949.1662; Average Loss: 0.6922925304359113; lr: [0.0001]\n",
      "Test Day 8, Epoch 133, Loss: 2945.4199; Average Loss: 0.6914131298871108; lr: [0.0001]\n",
      "Test Day 8, Epoch 134, Loss: 2947.9789; Average Loss: 0.6920138282954973; lr: [0.0001]\n",
      "Test Day 8, Epoch 135, Loss: 2951.3252; Average Loss: 0.6927993362498396; lr: [0.0001]\n",
      "Test Loss: 0.7034368872642517\n",
      "Test Day 9, Epoch 136, Loss: 2954.4613; Average Loss: 0.6927224712662512; lr: [0.0001]\n",
      "Test Day 9, Epoch 137, Loss: 2952.7593; Average Loss: 0.6923234078696856; lr: [0.0001]\n",
      "Test Day 9, Epoch 138, Loss: 2953.9809; Average Loss: 0.6926098357331432; lr: [0.0001]\n",
      "Test Day 9, Epoch 139, Loss: 2951.1159; Average Loss: 0.6919380755944381; lr: [0.0001]\n",
      "Test Day 9, Epoch 140, Loss: 2950.4819; Average Loss: 0.69178941465067; lr: [0.0001]\n",
      "Test Day 9, Epoch 141, Loss: 2950.2109; Average Loss: 0.6917258871957258; lr: [0.0001]\n",
      "Test Day 9, Epoch 142, Loss: 2951.1675; Average Loss: 0.6919501797674968; lr: [0.0001]\n",
      "Test Day 9, Epoch 143, Loss: 2953.5451; Average Loss: 0.6925076394120246; lr: [0.0001]\n",
      "Test Day 9, Epoch 144, Loss: 2953.2205; Average Loss: 0.6924315471582089; lr: [0.0001]\n",
      "Test Day 9, Epoch 145, Loss: 2949.5991; Average Loss: 0.6915824290875953; lr: [0.0001]\n",
      "Test Day 9, Epoch 146, Loss: 2955.2751; Average Loss: 0.6929132700804949; lr: [0.0001]\n",
      "Test Day 9, Epoch 147, Loss: 2956.5508; Average Loss: 0.6932123897503297; lr: [0.0001]\n",
      "Test Day 9, Epoch 148, Loss: 2951.0165; Average Loss: 0.6919147759780795; lr: [0.0001]\n",
      "Test Day 9, Epoch 149, Loss: 2951.3181; Average Loss: 0.6919854981398946; lr: [0.0001]\n",
      "Test Day 9, Epoch 150, Loss: 2954.8974; Average Loss: 0.6928247073890176; lr: [0.0001]\n",
      "Test Loss: 0.6887882113456726\n",
      "Test Day 10, Epoch 151, Loss: 2957.2724; Average Loss: 0.692569640202042; lr: [0.0001]\n",
      "Test Day 10, Epoch 152, Loss: 2954.6534; Average Loss: 0.6919563043312948; lr: [0.0001]\n",
      "Test Day 10, Epoch 153, Loss: 2955.7698; Average Loss: 0.6922177602870682; lr: [0.0001]\n",
      "Test Day 10, Epoch 154, Loss: 2958.1908; Average Loss: 0.692784731505347; lr: [0.0001]\n",
      "Test Day 10, Epoch 155, Loss: 2953.6070; Average Loss: 0.6917112381731878; lr: [0.0001]\n",
      "Test Day 10, Epoch 156, Loss: 2954.5985; Average Loss: 0.6919434478188008; lr: [0.0001]\n",
      "Test Day 10, Epoch 157, Loss: 2956.6974; Average Loss: 0.6924349912156545; lr: [0.0001]\n",
      "Test Day 10, Epoch 158, Loss: 2957.2048; Average Loss: 0.6925538270758242; lr: [0.0001]\n",
      "Test Day 10, Epoch 159, Loss: 2954.7882; Average Loss: 0.6919878778747988; lr: [0.0001]\n",
      "Test Day 10, Epoch 160, Loss: 2956.2445; Average Loss: 0.6923289341446387; lr: [0.0001]\n",
      "Test Day 10, Epoch 161, Loss: 2953.5389; Average Loss: 0.6916952910412112; lr: [0.0001]\n",
      "Test Day 10, Epoch 162, Loss: 2954.1703; Average Loss: 0.6918431534420969; lr: [0.0001]\n",
      "Test Day 10, Epoch 163, Loss: 2956.1809; Average Loss: 0.69231402689735; lr: [0.0001]\n",
      "Test Day 10, Epoch 164, Loss: 2955.7932; Average Loss: 0.6922232304021402; lr: [0.0001]\n",
      "Test Day 10, Epoch 165, Loss: 2953.5786; Average Loss: 0.6917045879140671; lr: [0.0001]\n",
      "Test Loss: 0.700380265712738\n",
      "Test Day 11, Epoch 166, Loss: 2960.2647; Average Loss: 0.6924595749169066; lr: [0.0001]\n",
      "Test Day 11, Epoch 167, Loss: 2962.4030; Average Loss: 0.692959759807029; lr: [0.0001]\n",
      "Test Day 11, Epoch 168, Loss: 2959.7142; Average Loss: 0.6923308050144485; lr: [0.0001]\n",
      "Test Day 11, Epoch 169, Loss: 2956.4058; Average Loss: 0.6915569015413697; lr: [0.0001]\n",
      "Test Day 11, Epoch 170, Loss: 2959.5074; Average Loss: 0.692282442684062; lr: [0.0001]\n",
      "Test Day 11, Epoch 171, Loss: 2957.9043; Average Loss: 0.6919074342961897; lr: [0.0001]\n",
      "Test Day 11, Epoch 172, Loss: 2956.7936; Average Loss: 0.6916476279810855; lr: [0.0001]\n",
      "Test Day 11, Epoch 173, Loss: 2957.4182; Average Loss: 0.6917937375788104; lr: [0.0001]\n",
      "Test Day 11, Epoch 174, Loss: 2958.1697; Average Loss: 0.6919695268597519; lr: [0.0001]\n",
      "Test Day 11, Epoch 175, Loss: 2958.0481; Average Loss: 0.69194106787966; lr: [0.0001]\n",
      "Test Day 11, Epoch 176, Loss: 2957.8586; Average Loss: 0.6918967549284997; lr: [0.0001]\n",
      "Test Day 11, Epoch 177, Loss: 2959.8781; Average Loss: 0.6923691456220303; lr: [0.0001]\n",
      "Test Day 11, Epoch 178, Loss: 2960.9627; Average Loss: 0.6926228635910658; lr: [0.0001]\n",
      "Test Day 11, Epoch 179, Loss: 2960.2441; Average Loss: 0.6924547545672858; lr: [0.0001]\n",
      "Test Day 11, Epoch 180, Loss: 2958.8295; Average Loss: 0.6921238708496094; lr: [0.0001]\n",
      "Test Loss: 0.6419855952262878\n",
      "Test Day 12, Epoch 181, Loss: 2960.7821; Average Loss: 0.6917715259801561; lr: [0.0001]\n",
      "Test Day 12, Epoch 182, Loss: 2960.7247; Average Loss: 0.691758112149818; lr: [0.0001]\n",
      "Test Day 12, Epoch 183, Loss: 2963.6190; Average Loss: 0.6924343528034531; lr: [0.0001]\n",
      "Test Day 12, Epoch 184, Loss: 2962.2259; Average Loss: 0.6921088459335755; lr: [0.0001]\n",
      "Test Day 12, Epoch 185, Loss: 2961.4224; Average Loss: 0.6919211209377396; lr: [0.0001]\n",
      "Test Day 12, Epoch 186, Loss: 2961.4697; Average Loss: 0.6919321755382503; lr: [0.0001]\n",
      "Test Day 12, Epoch 187, Loss: 2962.3962; Average Loss: 0.6921486373259642; lr: [0.0001]\n",
      "Test Day 12, Epoch 188, Loss: 2963.7618; Average Loss: 0.6924677100137016; lr: [0.0001]\n",
      "Test Day 12, Epoch 189, Loss: 2963.3326; Average Loss: 0.6923674271485516; lr: [0.0001]\n",
      "Test Day 12, Epoch 190, Loss: 2960.7735; Average Loss: 0.6917695009819815; lr: [0.0001]\n",
      "Test Day 12, Epoch 191, Loss: 2961.4111; Average Loss: 0.6919184782794703; lr: [0.0001]\n",
      "Test Day 12, Epoch 192, Loss: 2960.4377; Average Loss: 0.6916910438894112; lr: [0.0001]\n",
      "Test Day 12, Epoch 193, Loss: 2960.7026; Average Loss: 0.69175294626539; lr: [0.0001]\n",
      "Test Day 12, Epoch 194, Loss: 2961.5277; Average Loss: 0.6919457248438184; lr: [0.0001]\n",
      "Test Day 12, Epoch 195, Loss: 2963.1947; Average Loss: 0.6923352063259232; lr: [0.0001]\n",
      "Test Loss: 0.7231495380401611\n",
      "Test Day 13, Epoch 196, Loss: 2966.1873; Average Loss: 0.6922257377775853; lr: [0.0001]\n",
      "Test Day 13, Epoch 197, Loss: 2962.8438; Average Loss: 0.6914454501317847; lr: [0.0001]\n",
      "Test Day 13, Epoch 198, Loss: 2968.2935; Average Loss: 0.6927172648586776; lr: [0.0001]\n",
      "Test Day 13, Epoch 199, Loss: 2967.5001; Average Loss: 0.6925321011448685; lr: [0.0001]\n",
      "Test Day 13, Epoch 200, Loss: 2964.6995; Average Loss: 0.6918785397897941; lr: [0.0001]\n",
      "Test Day 13, Epoch 201, Loss: 2966.2644; Average Loss: 0.6922437429706302; lr: [0.0001]\n",
      "Test Day 13, Epoch 202, Loss: 2963.3121; Average Loss: 0.6915547472175946; lr: [0.0001]\n",
      "Test Day 13, Epoch 203, Loss: 2966.3105; Average Loss: 0.6922544944522817; lr: [0.0001]\n",
      "Test Day 13, Epoch 204, Loss: 2966.8486; Average Loss: 0.6923800705472396; lr: [0.0001]\n",
      "Test Day 13, Epoch 205, Loss: 2963.5353; Average Loss: 0.6916068309663752; lr: [0.0001]\n",
      "Test Day 13, Epoch 206, Loss: 2967.1564; Average Loss: 0.6924519097095609; lr: [0.0001]\n",
      "Test Day 13, Epoch 207, Loss: 2962.9174; Average Loss: 0.691462631848757; lr: [0.0001]\n",
      "Test Day 13, Epoch 208, Loss: 2967.4671; Average Loss: 0.6925244112137179; lr: [0.0001]\n",
      "Test Day 13, Epoch 209, Loss: 2963.7240; Average Loss: 0.6916508864911323; lr: [0.0001]\n",
      "Test Day 13, Epoch 210, Loss: 2964.4637; Average Loss: 0.6918235031003296; lr: [0.0001]\n",
      "Test Loss: 0.6804942488670349\n",
      "Test Day 14, Epoch 211, Loss: 2967.3470; Average Loss: 0.6916892680810603; lr: [0.0001]\n",
      "Test Day 14, Epoch 212, Loss: 2968.3155; Average Loss: 0.6919150288010533; lr: [0.0001]\n",
      "Test Day 14, Epoch 213, Loss: 2969.8547; Average Loss: 0.6922738304782858; lr: [0.0001]\n",
      "Test Day 14, Epoch 214, Loss: 2967.2362; Average Loss: 0.6916634501555027; lr: [0.0001]\n",
      "Test Day 14, Epoch 215, Loss: 2968.8617; Average Loss: 0.6920423511302832; lr: [0.0001]\n",
      "Test Day 14, Epoch 216, Loss: 2972.3960; Average Loss: 0.6928662066415195; lr: [0.0001]\n",
      "Test Day 14, Epoch 217, Loss: 2967.9099; Average Loss: 0.6918204836634211; lr: [0.0001]\n",
      "Test Day 14, Epoch 218, Loss: 2968.2645; Average Loss: 0.6919031406060243; lr: [0.0001]\n",
      "Test Day 14, Epoch 219, Loss: 2968.4988; Average Loss: 0.6919577542996351; lr: [0.0001]\n",
      "Test Day 14, Epoch 220, Loss: 2967.6710; Average Loss: 0.6917648100908542; lr: [0.0001]\n",
      "Test Day 14, Epoch 221, Loss: 2970.1093; Average Loss: 0.6923331649987015; lr: [0.0001]\n",
      "Test Day 14, Epoch 222, Loss: 2969.6601; Average Loss: 0.692228452420179; lr: [0.0001]\n",
      "Test Day 14, Epoch 223, Loss: 2969.1672; Average Loss: 0.6921135600074466; lr: [0.0001]\n",
      "Test Day 14, Epoch 224, Loss: 2966.7829; Average Loss: 0.691557780123535; lr: [0.0001]\n",
      "Test Day 14, Epoch 225, Loss: 2969.6063; Average Loss: 0.6922159268305852; lr: [0.0001]\n",
      "Test Loss: 0.6995797157287598\n",
      "Test Day 15, Epoch 226, Loss: 2969.6232; Average Loss: 0.6914140252767259; lr: [0.0001]\n",
      "Test Day 15, Epoch 227, Loss: 2971.1393; Average Loss: 0.6917670041218625; lr: [0.0001]\n",
      "Test Day 15, Epoch 228, Loss: 2973.9805; Average Loss: 0.6924285235865707; lr: [0.0001]\n",
      "Test Day 15, Epoch 229, Loss: 2970.5232; Average Loss: 0.6916235669527954; lr: [0.0001]\n",
      "Test Day 15, Epoch 230, Loss: 2973.4903; Average Loss: 0.6923143880886304; lr: [0.0001]\n",
      "Test Day 15, Epoch 231, Loss: 2972.1123; Average Loss: 0.6919935517316648; lr: [0.0001]\n",
      "Test Day 15, Epoch 232, Loss: 2970.7019; Average Loss: 0.6916651702454537; lr: [0.0001]\n",
      "Test Day 15, Epoch 233, Loss: 2974.5724; Average Loss: 0.6925663309685703; lr: [0.0001]\n",
      "Test Day 15, Epoch 234, Loss: 2970.6622; Average Loss: 0.691655922158078; lr: [0.0001]\n",
      "Test Day 15, Epoch 235, Loss: 2973.6086; Average Loss: 0.6923419386183147; lr: [0.0001]\n",
      "Test Day 15, Epoch 236, Loss: 2972.1344; Average Loss: 0.6919986890275487; lr: [0.0001]\n",
      "Test Day 15, Epoch 237, Loss: 2971.6034; Average Loss: 0.6918750556439542; lr: [0.0001]\n",
      "Test Day 15, Epoch 238, Loss: 2972.6646; Average Loss: 0.6921221513270777; lr: [0.0001]\n",
      "Test Day 15, Epoch 239, Loss: 2972.1580; Average Loss: 0.6920041995775713; lr: [0.0001]\n",
      "Test Day 15, Epoch 240, Loss: 2969.8718; Average Loss: 0.6914718980977921; lr: [0.0001]\n",
      "Test Loss: 0.6648846745491028\n",
      "Test Day 16, Epoch 241, Loss: 2976.1342; Average Loss: 0.6921242288101551; lr: [0.0001]\n",
      "Test Day 16, Epoch 242, Loss: 2976.7028; Average Loss: 0.692256458859111; lr: [0.0001]\n",
      "Test Day 16, Epoch 243, Loss: 2975.7414; Average Loss: 0.6920328894326854; lr: [0.0001]\n",
      "Test Day 16, Epoch 244, Loss: 2977.0836; Average Loss: 0.6923450245968131; lr: [0.0001]\n",
      "Test Day 16, Epoch 245, Loss: 2976.1877; Average Loss: 0.6921366806917413; lr: [0.0001]\n",
      "Test Day 16, Epoch 246, Loss: 2975.6027; Average Loss: 0.69200063173161; lr: [0.0001]\n",
      "Test Day 16, Epoch 247, Loss: 2979.6250; Average Loss: 0.6929360354223917; lr: [0.0001]\n",
      "Test Day 16, Epoch 248, Loss: 2977.9758; Average Loss: 0.6925525008800418; lr: [0.0001]\n",
      "Test Day 16, Epoch 249, Loss: 2976.2518; Average Loss: 0.6921515919441401; lr: [0.0001]\n",
      "Test Day 16, Epoch 250, Loss: 2978.4903; Average Loss: 0.6926721550697504; lr: [0.0001]\n",
      "Test Day 16, Epoch 251, Loss: 2977.5765; Average Loss: 0.6924596498178881; lr: [0.0001]\n",
      "Test Day 16, Epoch 252, Loss: 2977.8818; Average Loss: 0.69253064643505; lr: [0.0001]\n",
      "Test Day 16, Epoch 253, Loss: 2977.3911; Average Loss: 0.6924165277702864; lr: [0.0001]\n",
      "Test Day 16, Epoch 254, Loss: 2975.0376; Average Loss: 0.6918692162979481; lr: [0.0001]\n",
      "Test Day 16, Epoch 255, Loss: 2977.1055; Average Loss: 0.6923501232058503; lr: [0.0001]\n",
      "Test Loss: 0.6818657040596008\n",
      "Test Day 17, Epoch 256, Loss: 2978.4836; Average Loss: 0.6918661013435957; lr: [0.0001]\n",
      "Test Day 17, Epoch 257, Loss: 2976.9228; Average Loss: 0.6915035546864367; lr: [0.0001]\n",
      "Test Day 17, Epoch 258, Loss: 2977.4352; Average Loss: 0.6916225747918141; lr: [0.0001]\n",
      "Test Day 17, Epoch 259, Loss: 2980.3026; Average Loss: 0.6922886322322761; lr: [0.0001]\n",
      "Test Day 17, Epoch 260, Loss: 2979.6334; Average Loss: 0.6921332036991983; lr: [0.0001]\n",
      "Test Day 17, Epoch 261, Loss: 2981.2132; Average Loss: 0.6925001598539806; lr: [0.0001]\n",
      "Test Day 17, Epoch 262, Loss: 2978.9622; Average Loss: 0.6919772816036637; lr: [0.0001]\n",
      "Test Day 17, Epoch 263, Loss: 2977.6926; Average Loss: 0.6916823587628054; lr: [0.0001]\n",
      "Test Day 17, Epoch 264, Loss: 2977.2473; Average Loss: 0.6915789266358131; lr: [0.0001]\n",
      "Test Day 17, Epoch 265, Loss: 2973.8230; Average Loss: 0.6907835097539993; lr: [0.0001]\n",
      "Test Day 17, Epoch 266, Loss: 2980.0619; Average Loss: 0.6922327300812725; lr: [0.0001]\n",
      "Test Day 17, Epoch 267, Loss: 2979.5748; Average Loss: 0.6921195815526097; lr: [0.0001]\n",
      "Test Day 17, Epoch 268, Loss: 2978.0279; Average Loss: 0.6917602574506841; lr: [0.0001]\n",
      "Test Day 17, Epoch 269, Loss: 2979.5228; Average Loss: 0.6921074959182296; lr: [0.0001]\n",
      "Test Day 17, Epoch 270, Loss: 2974.9764; Average Loss: 0.691051430873893; lr: [0.0001]\n",
      "Test Loss: 0.7289565920829773\n",
      "Test Day 18, Epoch 271, Loss: 2982.0583; Average Loss: 0.6918928832299748; lr: [0.0001]\n",
      "Test Day 18, Epoch 272, Loss: 2983.1618; Average Loss: 0.6921489188953232; lr: [0.0001]\n",
      "Test Day 18, Epoch 273, Loss: 2984.4262; Average Loss: 0.6924422815061777; lr: [0.0001]\n",
      "Test Day 18, Epoch 274, Loss: 2980.2559; Average Loss: 0.6914746773491879; lr: [0.0001]\n",
      "Test Day 18, Epoch 275, Loss: 2982.8598; Average Loss: 0.6920788284797403; lr: [0.0001]\n",
      "Test Day 18, Epoch 276, Loss: 2981.1080; Average Loss: 0.69167239107278; lr: [0.0001]\n",
      "Test Day 18, Epoch 277, Loss: 2981.7772; Average Loss: 0.691827646153709; lr: [0.0001]\n",
      "Test Day 18, Epoch 278, Loss: 2984.7944; Average Loss: 0.6925276966493257; lr: [0.0001]\n",
      "Test Day 18, Epoch 279, Loss: 2981.8763; Average Loss: 0.6918506387768102; lr: [0.0001]\n",
      "Test Day 18, Epoch 280, Loss: 2983.7686; Average Loss: 0.692289709547003; lr: [0.0001]\n",
      "Test Day 18, Epoch 281, Loss: 2984.5896; Average Loss: 0.6924801957026435; lr: [0.0001]\n",
      "Test Day 18, Epoch 282, Loss: 2981.5305; Average Loss: 0.6917704110908951; lr: [0.0001]\n",
      "Test Day 18, Epoch 283, Loss: 2984.8309; Average Loss: 0.6925361788189881; lr: [0.0001]\n",
      "Test Day 18, Epoch 284, Loss: 2983.5490; Average Loss: 0.692238755679739; lr: [0.0001]\n",
      "Test Day 18, Epoch 285, Loss: 2981.7720; Average Loss: 0.6918264442142921; lr: [0.0001]\n",
      "Test Loss: 0.7022155165672302\n",
      "Test Day 19, Epoch 286, Loss: 2985.8154; Average Loss: 0.6919618476169273; lr: [0.0001]\n",
      "Test Day 19, Epoch 287, Loss: 2984.6839; Average Loss: 0.6916996404442285; lr: [0.0001]\n",
      "Test Day 19, Epoch 288, Loss: 2984.1085; Average Loss: 0.6915662909161865; lr: [0.0001]\n",
      "Test Day 19, Epoch 289, Loss: 2986.3109; Average Loss: 0.6920766881277498; lr: [0.0001]\n",
      "Test Day 19, Epoch 290, Loss: 2984.6434; Average Loss: 0.6916902482440547; lr: [0.0001]\n",
      "Test Day 19, Epoch 291, Loss: 2985.2841; Average Loss: 0.6918387173224987; lr: [0.0001]\n",
      "Test Day 19, Epoch 292, Loss: 2986.2029; Average Loss: 0.6920516463862095; lr: [0.0001]\n",
      "Test Day 19, Epoch 293, Loss: 2985.7844; Average Loss: 0.691954663786109; lr: [0.0001]\n",
      "Test Day 19, Epoch 294, Loss: 2985.4818; Average Loss: 0.6918845383344631; lr: [0.0001]\n",
      "Test Day 19, Epoch 295, Loss: 2985.1605; Average Loss: 0.6918100845496044; lr: [0.0001]\n",
      "Test Day 19, Epoch 296, Loss: 2985.1317; Average Loss: 0.6918034170068982; lr: [0.0001]\n",
      "Test Day 19, Epoch 297, Loss: 2983.9586; Average Loss: 0.6915315435769799; lr: [0.0001]\n",
      "Test Day 19, Epoch 298, Loss: 2986.1999; Average Loss: 0.6920509621276502; lr: [0.0001]\n",
      "Test Day 19, Epoch 299, Loss: 2982.6807; Average Loss: 0.6912353939971393; lr: [0.0001]\n",
      "Test Day 19, Epoch 300, Loss: 2985.6042; Average Loss: 0.6919129098691089; lr: [0.0001]\n",
      "Test Loss: 0.698846447467804\n",
      "Test Day 20, Epoch 301, Loss: 2989.7144; Average Loss: 0.6920635254294784; lr: [0.0001]\n",
      "Test Day 20, Epoch 302, Loss: 2991.3891; Average Loss: 0.6924511790275574; lr: [0.0001]\n",
      "Test Day 20, Epoch 303, Loss: 2987.6427; Average Loss: 0.6915839601446081; lr: [0.0001]\n",
      "Test Day 20, Epoch 304, Loss: 2987.4364; Average Loss: 0.6915361925407693; lr: [0.0001]\n",
      "Test Day 20, Epoch 305, Loss: 2987.6960; Average Loss: 0.6915962916833384; lr: [0.0001]\n",
      "Test Day 20, Epoch 306, Loss: 2989.7989; Average Loss: 0.6920830832587348; lr: [0.0001]\n",
      "Test Day 20, Epoch 307, Loss: 2987.0176; Average Loss: 0.6914392603768242; lr: [0.0001]\n",
      "Test Day 20, Epoch 308, Loss: 2988.1615; Average Loss: 0.6917040533489651; lr: [0.0001]\n",
      "Test Day 20, Epoch 309, Loss: 2989.3406; Average Loss: 0.6919769887570981; lr: [0.0001]\n",
      "Test Day 20, Epoch 310, Loss: 2989.0615; Average Loss: 0.6919123888015747; lr: [0.0001]\n",
      "Test Day 20, Epoch 311, Loss: 2989.6276; Average Loss: 0.6920434187959742; lr: [0.0001]\n",
      "Test Day 20, Epoch 312, Loss: 2990.5244; Average Loss: 0.6922510200076633; lr: [0.0001]\n",
      "Test Day 20, Epoch 313, Loss: 2987.5090; Average Loss: 0.6915530156206202; lr: [0.0001]\n",
      "Test Day 20, Epoch 314, Loss: 2989.8372; Average Loss: 0.6920919405089484; lr: [0.0001]\n",
      "Test Day 20, Epoch 315, Loss: 2986.8897; Average Loss: 0.6914096549705223; lr: [0.0001]\n",
      "Test Loss: 0.7345137119293212\n",
      "Test Day 21, Epoch 316, Loss: 2993.9884; Average Loss: 0.6922516495368384; lr: [0.0001]\n",
      "Test Day 21, Epoch 317, Loss: 2991.8652; Average Loss: 0.6917607451863371; lr: [0.0001]\n",
      "Test Day 21, Epoch 318, Loss: 2993.1128; Average Loss: 0.692049190984296; lr: [0.0001]\n",
      "Test Day 21, Epoch 319, Loss: 2992.2087; Average Loss: 0.6918401702704458; lr: [0.0001]\n",
      "Test Day 21, Epoch 320, Loss: 2993.3308; Average Loss: 0.6920996084929891; lr: [0.0001]\n",
      "Test Day 21, Epoch 321, Loss: 2990.7754; Average Loss: 0.6915087608381503; lr: [0.0001]\n",
      "Test Day 21, Epoch 322, Loss: 2989.3583; Average Loss: 0.6911811065673829; lr: [0.0001]\n",
      "Test Day 21, Epoch 323, Loss: 2992.1233; Average Loss: 0.6918204119026317; lr: [0.0001]\n",
      "Test Day 21, Epoch 324, Loss: 2989.3248; Average Loss: 0.6911733695675183; lr: [0.0001]\n",
      "Test Day 21, Epoch 325, Loss: 2991.1765; Average Loss: 0.6916015113433661; lr: [0.0001]\n",
      "Test Day 21, Epoch 326, Loss: 2991.5754; Average Loss: 0.691693725144932; lr: [0.0001]\n",
      "Test Day 21, Epoch 327, Loss: 2992.7136; Average Loss: 0.6919569079448722; lr: [0.0001]\n",
      "Test Day 21, Epoch 328, Loss: 2991.9228; Average Loss: 0.6917740573772805; lr: [0.0001]\n",
      "Test Day 21, Epoch 329, Loss: 2993.1706; Average Loss: 0.6920625613879606; lr: [0.0001]\n",
      "Test Day 21, Epoch 330, Loss: 2993.9433; Average Loss: 0.6922412338697841; lr: [0.0001]\n",
      "Test Loss: 0.6520200848579407\n",
      "Test Day 22, Epoch 331, Loss: 2996.2603; Average Loss: 0.6919769648316275; lr: [0.0001]\n",
      "Test Day 22, Epoch 332, Loss: 2996.4349; Average Loss: 0.6920173032729785; lr: [0.0001]\n",
      "Test Day 22, Epoch 333, Loss: 2996.0445; Average Loss: 0.6919271279702837; lr: [0.0001]\n",
      "Test Day 22, Epoch 334, Loss: 2994.6521; Average Loss: 0.6916055767420533; lr: [0.0001]\n",
      "Test Day 22, Epoch 335, Loss: 2995.6449; Average Loss: 0.6918348497386342; lr: [0.0001]\n",
      "Test Day 22, Epoch 336, Loss: 2995.9326; Average Loss: 0.6919012941884665; lr: [0.0001]\n",
      "Test Day 22, Epoch 337, Loss: 2997.2875; Average Loss: 0.6922141993568897; lr: [0.0001]\n",
      "Test Day 22, Epoch 338, Loss: 2996.5210; Average Loss: 0.6920371934392986; lr: [0.0001]\n",
      "Test Day 22, Epoch 339, Loss: 2998.1253; Average Loss: 0.6924076855595613; lr: [0.0001]\n",
      "Test Day 22, Epoch 340, Loss: 2995.3629; Average Loss: 0.6917697285394471; lr: [0.0001]\n",
      "Test Day 22, Epoch 341, Loss: 2994.2191; Average Loss: 0.6915055726472005; lr: [0.0001]\n",
      "Test Day 22, Epoch 342, Loss: 2996.5026; Average Loss: 0.6920329316383697; lr: [0.0001]\n",
      "Test Day 22, Epoch 343, Loss: 2992.8424; Average Loss: 0.6911876282020198; lr: [0.0001]\n",
      "Test Day 22, Epoch 344, Loss: 2995.4994; Average Loss: 0.6918012464844878; lr: [0.0001]\n",
      "Test Day 22, Epoch 345, Loss: 2995.5975; Average Loss: 0.6918239029670697; lr: [0.0001]\n",
      "Test Loss: 0.715354323387146\n",
      "Test Day 23, Epoch 346, Loss: 3001.1071; Average Loss: 0.6922969039336208; lr: [0.0001]\n",
      "Test Day 23, Epoch 347, Loss: 3002.0450; Average Loss: 0.6925132628268161; lr: [0.0001]\n",
      "Test Day 23, Epoch 348, Loss: 3000.1587; Average Loss: 0.6920781207057156; lr: [0.0001]\n",
      "Test Day 23, Epoch 349, Loss: 2997.7660; Average Loss: 0.6915261714103725; lr: [0.0001]\n",
      "Test Day 23, Epoch 350, Loss: 3000.2950; Average Loss: 0.6921095745786251; lr: [0.0001]\n",
      "Test Day 23, Epoch 351, Loss: 2998.0773; Average Loss: 0.6915979880362646; lr: [0.0001]\n",
      "Test Day 23, Epoch 352, Loss: 2999.5462; Average Loss: 0.691936835230703; lr: [0.0001]\n",
      "Test Day 23, Epoch 353, Loss: 3000.4674; Average Loss: 0.6921493525873555; lr: [0.0001]\n",
      "Test Day 23, Epoch 354, Loss: 2997.2072; Average Loss: 0.6913972883213487; lr: [0.0001]\n",
      "Test Day 23, Epoch 355, Loss: 3002.5866; Average Loss: 0.6926382036220106; lr: [0.0001]\n",
      "Test Day 23, Epoch 356, Loss: 2998.9941; Average Loss: 0.6918094815404907; lr: [0.0001]\n",
      "Test Day 23, Epoch 357, Loss: 2995.5997; Average Loss: 0.691026455748315; lr: [0.0001]\n",
      "Test Day 23, Epoch 358, Loss: 2997.6607; Average Loss: 0.6915019007840096; lr: [0.0001]\n",
      "Test Day 23, Epoch 359, Loss: 3001.1650; Average Loss: 0.6923102725335883; lr: [0.0001]\n",
      "Test Day 23, Epoch 360, Loss: 2998.8744; Average Loss: 0.691781877563899; lr: [0.0001]\n",
      "Test Loss: 0.6509485602378845\n",
      "Test Day 24, Epoch 361, Loss: 3005.6983; Average Loss: 0.6925572048134518; lr: [0.0001]\n",
      "Test Day 24, Epoch 362, Loss: 3000.1329; Average Loss: 0.6912748714745869; lr: [0.0001]\n",
      "Test Day 24, Epoch 363, Loss: 3002.5698; Average Loss: 0.6918363650273618; lr: [0.0001]\n",
      "Test Day 24, Epoch 364, Loss: 3001.1620; Average Loss: 0.6915119883102206; lr: [0.0001]\n",
      "Test Day 24, Epoch 365, Loss: 3006.8595; Average Loss: 0.6928247750629478; lr: [0.0001]\n",
      "Test Day 24, Epoch 366, Loss: 3001.4828; Average Loss: 0.6915858879616733; lr: [0.0001]\n",
      "Test Day 24, Epoch 367, Loss: 3004.0840; Average Loss: 0.6921852621614658; lr: [0.0001]\n",
      "Test Day 24, Epoch 368, Loss: 3002.2055; Average Loss: 0.691752424108268; lr: [0.0001]\n",
      "Test Day 24, Epoch 369, Loss: 2998.9059; Average Loss: 0.6909921523063414; lr: [0.0001]\n",
      "Test Day 24, Epoch 370, Loss: 3001.7178; Average Loss: 0.6916400513890701; lr: [0.0001]\n",
      "Test Day 24, Epoch 371, Loss: 3003.0466; Average Loss: 0.6919462239137992; lr: [0.0001]\n",
      "Test Day 24, Epoch 372, Loss: 3003.9971; Average Loss: 0.6921652235743088; lr: [0.0001]\n",
      "Test Day 24, Epoch 373, Loss: 3000.8686; Average Loss: 0.6914443767565187; lr: [0.0001]\n",
      "Test Day 24, Epoch 374, Loss: 3005.6081; Average Loss: 0.6925364305346792; lr: [0.0001]\n",
      "Test Day 24, Epoch 375, Loss: 3002.4461; Average Loss: 0.691807858515445; lr: [0.0001]\n",
      "Test Loss: 0.6510544538497924\n",
      "Test Day 25, Epoch 376, Loss: 3006.9646; Average Loss: 0.6920516967773438; lr: [0.0001]\n",
      "Test Day 25, Epoch 377, Loss: 3003.7312; Average Loss: 0.691307519137928; lr: [0.0001]\n",
      "Test Day 25, Epoch 378, Loss: 3006.7574; Average Loss: 0.6920040003312333; lr: [0.0001]\n",
      "Test Day 25, Epoch 379, Loss: 3006.6890; Average Loss: 0.6919882648147565; lr: [0.0001]\n",
      "Test Day 25, Epoch 380, Loss: 3006.1939; Average Loss: 0.6918743137934678; lr: [0.0001]\n",
      "Test Day 25, Epoch 381, Loss: 3003.4442; Average Loss: 0.6912414735150968; lr: [0.0001]\n",
      "Test Day 25, Epoch 382, Loss: 3005.7239; Average Loss: 0.6917661467136256; lr: [0.0001]\n",
      "Test Day 25, Epoch 383, Loss: 3004.7398; Average Loss: 0.6915396564163191; lr: [0.0001]\n",
      "Test Day 25, Epoch 384, Loss: 3004.0631; Average Loss: 0.691383916681463; lr: [0.0001]\n",
      "Test Day 25, Epoch 385, Loss: 3004.4152; Average Loss: 0.6914649454167303; lr: [0.0001]\n",
      "Test Day 25, Epoch 386, Loss: 3005.0000; Average Loss: 0.6915995326771972; lr: [0.0001]\n",
      "Test Day 25, Epoch 387, Loss: 3002.9175; Average Loss: 0.6911202521812518; lr: [0.0001]\n",
      "Test Day 25, Epoch 388, Loss: 3004.7503; Average Loss: 0.6915420804391385; lr: [0.0001]\n",
      "Test Day 25, Epoch 389, Loss: 3005.1687; Average Loss: 0.6916383714752724; lr: [0.0001]\n",
      "Test Day 25, Epoch 390, Loss: 3008.4038; Average Loss: 0.6923829134643695; lr: [0.0001]\n",
      "Test Loss: 0.7344481945037842\n",
      "Test Day 26, Epoch 391, Loss: 3010.5218; Average Loss: 0.692073983378794; lr: [0.0001]\n",
      "Test Day 26, Epoch 392, Loss: 3008.6681; Average Loss: 0.6916478474934896; lr: [0.0001]\n",
      "Test Day 26, Epoch 393, Loss: 3009.2263; Average Loss: 0.6917761677709119; lr: [0.0001]\n",
      "Test Day 26, Epoch 394, Loss: 3008.1250; Average Loss: 0.691522998152108; lr: [0.0001]\n",
      "Test Day 26, Epoch 395, Loss: 3008.4623; Average Loss: 0.6916005189391389; lr: [0.0001]\n",
      "Test Day 26, Epoch 396, Loss: 3009.4000; Average Loss: 0.6918160984433931; lr: [0.0001]\n",
      "Test Day 26, Epoch 397, Loss: 3007.6809; Average Loss: 0.6914209019452676; lr: [0.0001]\n",
      "Test Day 26, Epoch 398, Loss: 3010.6482; Average Loss: 0.6921030373408876; lr: [0.0001]\n",
      "Test Day 26, Epoch 399, Loss: 3010.4713; Average Loss: 0.6920623665294428; lr: [0.0001]\n",
      "Test Day 26, Epoch 400, Loss: 3007.0760; Average Loss: 0.6912818329909752; lr: [0.0001]\n",
      "Test Day 26, Epoch 401, Loss: 3007.7753; Average Loss: 0.6914425974878772; lr: [0.0001]\n",
      "Test Day 26, Epoch 402, Loss: 3013.7468; Average Loss: 0.6928153474303498; lr: [0.0001]\n",
      "Test Day 26, Epoch 403, Loss: 3007.1509; Average Loss: 0.6912990499913008; lr: [0.0001]\n",
      "Test Day 26, Epoch 404, Loss: 3010.0730; Average Loss: 0.6919708137950678; lr: [0.0001]\n",
      "Test Day 26, Epoch 405, Loss: 3010.0895; Average Loss: 0.6919745995532507; lr: [0.0001]\n",
      "Test Loss: 0.6717063784599304\n",
      "Test Day 27, Epoch 406, Loss: 3013.4960; Average Loss: 0.6919623437896799; lr: [0.0001]\n",
      "Test Day 27, Epoch 407, Loss: 3012.1784; Average Loss: 0.6916597897094919; lr: [0.0001]\n",
      "Test Day 27, Epoch 408, Loss: 3011.5317; Average Loss: 0.6915112880833798; lr: [0.0001]\n",
      "Test Day 27, Epoch 409, Loss: 3012.8517; Average Loss: 0.6918143916212185; lr: [0.0001]\n",
      "Test Day 27, Epoch 410, Loss: 3010.3766; Average Loss: 0.691246067697501; lr: [0.0001]\n",
      "Test Day 27, Epoch 411, Loss: 3010.9691; Average Loss: 0.6913821055612662; lr: [0.0001]\n",
      "Test Day 27, Epoch 412, Loss: 3012.3667; Average Loss: 0.6917030347611682; lr: [0.0001]\n",
      "Test Day 27, Epoch 413, Loss: 3015.5249; Average Loss: 0.6924282140764659; lr: [0.0001]\n",
      "Test Day 27, Epoch 414, Loss: 3012.3458; Average Loss: 0.691698227684205; lr: [0.0001]\n",
      "Test Day 27, Epoch 415, Loss: 3011.6613; Average Loss: 0.6915410515908943; lr: [0.0001]\n",
      "Test Day 27, Epoch 416, Loss: 3011.1295; Average Loss: 0.6914189534674556; lr: [0.0001]\n",
      "Test Day 27, Epoch 417, Loss: 3014.8985; Average Loss: 0.6922843751622669; lr: [0.0001]\n",
      "Test Day 27, Epoch 418, Loss: 3015.0220; Average Loss: 0.6923127544186283; lr: [0.0001]\n",
      "Test Day 27, Epoch 419, Loss: 3011.8519; Average Loss: 0.691584816099445; lr: [0.0001]\n",
      "Test Day 27, Epoch 420, Loss: 3012.7553; Average Loss: 0.6917922559753488; lr: [0.0001]\n",
      "Test Loss: 0.6667136669158935\n",
      "Test Day 28, Epoch 421, Loss: 3012.7764; Average Loss: 0.6910037683784415; lr: [0.0001]\n",
      "Test Day 28, Epoch 422, Loss: 3012.8797; Average Loss: 0.6910274645604125; lr: [0.0001]\n",
      "Test Day 28, Epoch 423, Loss: 3020.3333; Average Loss: 0.6927369852678491; lr: [0.0001]\n",
      "Test Day 28, Epoch 424, Loss: 3017.4132; Average Loss: 0.6920672434185623; lr: [0.0001]\n",
      "Test Day 28, Epoch 425, Loss: 3016.1915; Average Loss: 0.6917870521545411; lr: [0.0001]\n",
      "Test Day 28, Epoch 426, Loss: 3012.4349; Average Loss: 0.6909254466721771; lr: [0.0001]\n",
      "Test Day 28, Epoch 427, Loss: 3015.9857; Average Loss: 0.6917398332455836; lr: [0.0001]\n",
      "Test Day 28, Epoch 428, Loss: 3014.8287; Average Loss: 0.6914744829912798; lr: [0.0001]\n",
      "Test Day 28, Epoch 429, Loss: 3013.6433; Average Loss: 0.6912025989742454; lr: [0.0001]\n",
      "Test Day 28, Epoch 430, Loss: 3014.8742; Average Loss: 0.691484898602197; lr: [0.0001]\n",
      "Test Day 28, Epoch 431, Loss: 3014.5891; Average Loss: 0.6914195260870347; lr: [0.0001]\n",
      "Test Day 28, Epoch 432, Loss: 3015.9199; Average Loss: 0.6917247474740404; lr: [0.0001]\n",
      "Test Day 28, Epoch 433, Loss: 3012.6897; Average Loss: 0.6909838746447082; lr: [0.0001]\n",
      "Test Day 28, Epoch 434, Loss: 3019.5170; Average Loss: 0.6925497635788874; lr: [0.0001]\n",
      "Test Day 28, Epoch 435, Loss: 3018.6593; Average Loss: 0.69235303937842; lr: [0.0001]\n",
      "Test Loss: 0.685165798664093\n",
      "Test Day 29, Epoch 436, Loss: 3019.0973; Average Loss: 0.6916603269719039; lr: [0.0001]\n",
      "Test Day 29, Epoch 437, Loss: 3016.2011; Average Loss: 0.690996808448608; lr: [0.0001]\n",
      "Test Day 29, Epoch 438, Loss: 3015.2197; Average Loss: 0.6907719772705917; lr: [0.0001]\n",
      "Test Day 29, Epoch 439, Loss: 3018.2777; Average Loss: 0.6914725531963685; lr: [0.0001]\n",
      "Test Day 29, Epoch 440, Loss: 3019.6510; Average Loss: 0.6917871774540187; lr: [0.0001]\n",
      "Test Day 29, Epoch 441, Loss: 3017.1606; Average Loss: 0.6912166398825105; lr: [0.0001]\n",
      "Test Day 29, Epoch 442, Loss: 3017.9941; Average Loss: 0.6914075788080624; lr: [0.0001]\n",
      "Test Day 29, Epoch 443, Loss: 3018.9302; Average Loss: 0.6916220373304439; lr: [0.0001]\n",
      "Test Day 29, Epoch 444, Loss: 3020.4684; Average Loss: 0.6919744428217343; lr: [0.0001]\n",
      "Test Day 29, Epoch 445, Loss: 3017.8170; Average Loss: 0.6913670055918131; lr: [0.0001]\n",
      "Test Day 29, Epoch 446, Loss: 3020.5173; Average Loss: 0.6919856452723413; lr: [0.0001]\n",
      "Test Day 29, Epoch 447, Loss: 3019.6980; Average Loss: 0.6917979416295562; lr: [0.0001]\n",
      "Test Day 29, Epoch 448, Loss: 3023.2920; Average Loss: 0.6926213169425623; lr: [0.0001]\n",
      "Test Day 29, Epoch 449, Loss: 3020.5282; Average Loss: 0.6919881296321698; lr: [0.0001]\n",
      "Test Day 29, Epoch 450, Loss: 3021.5104; Average Loss: 0.6922131502341569; lr: [0.0001]\n",
      "Test Loss: 0.6692997574806213\n"
     ]
    }
   ],
   "source": [
    "true_labels, predicted_scores, companies = agent.train(30, 15) # one column for each company ticker, flatten to compute ROC over all\n",
    "fpr, tpr, thresholds = roc_curve(true_labels.reshape(-1), predicted_scores.reshape(-1))\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
