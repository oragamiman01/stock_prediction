{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from StockDataWrapper import get_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthDataset(Dataset):\n",
    "    def __init__(self, tickers: list[str], data_dir: str, lag_days: int, test_days: int, test: bool):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.tickers = []\n",
    "\n",
    "        num_to_ticker = dict()\n",
    "        ticker_to_num = dict()\n",
    "        ids = np.linspace(-1, 1, len(tickers)).round(5)\n",
    "        for i, num in enumerate(ids):\n",
    "            num_to_ticker[str(num)] = tickers[i]\n",
    "            ticker_to_num[tickers[i]] = num\n",
    "\n",
    "        # get the unix timestamp value for 2024-04-15. Used to scale the date value used as input to the model\n",
    "        cur_unix_timestamp = datetime.datetime(year=2024, month=4, day=15).timestamp()\n",
    "\n",
    "        for ticker in tickers:\n",
    "            ordered_time_series = get_time_series(ticker, data_dir, normalize=True)\n",
    "\n",
    "            if test:\n",
    "                idx = range(len(ordered_time_series) - lag_days - test_days, len(ordered_time_series))\n",
    "            else:\n",
    "                idx = range(lag_days, len(ordered_time_series) - lag_days - test_days)\n",
    "\n",
    "            # need to get lag_days days of input tokens and predict the lag_days+1 day, and so on\n",
    "            for cur_target_idx in idx:\n",
    "\n",
    "                # get target values\n",
    "                cur_target_actual_after = [ticker_to_num[ticker]]\n",
    "                for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                    val = ordered_time_series[cur_target_idx][key]\n",
    "                    if key == 'date':\n",
    "                        val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                    cur_target_actual_after.append(val)\n",
    "\n",
    "                # each \"sample\" (input) will have multiple tokens. Each token is a vector of the day's values\n",
    "                cur_input = []\n",
    "\n",
    "                for i in range(cur_target_idx - lag_days, cur_target_idx):\n",
    "\n",
    "                    cur_token = [ticker_to_num[ticker]]\n",
    "                    for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                        val = ordered_time_series[cur_target_idx][key]\n",
    "                        if key == 'date':\n",
    "                            val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                        cur_token.append(val)\n",
    "\n",
    "                    cur_input.append(cur_token)\n",
    "\n",
    "                self.X.append(cur_input)\n",
    "                self.y.append(cur_target_actual_after)\n",
    "                self.tickers.append(ticker)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32), self.tickers[idx]\n",
    "    \n",
    "\n",
    "class IncrementalDataset(Dataset):\n",
    "    def __init__(self, tickers: list[str], data_dir: str, lag_days: int, test_days: int, test: bool):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.tickers = []\n",
    "\n",
    "        num_to_ticker = dict()\n",
    "        ticker_to_num = dict()\n",
    "        ids = np.linspace(-1, 1, len(tickers)).round(5)\n",
    "        for i, num in enumerate(ids):\n",
    "            num_to_ticker[str(num)] = tickers[i]\n",
    "            ticker_to_num[tickers[i]] = num\n",
    "\n",
    "        # get the unix timestamp value for 2024-04-15. Used to scale the date value used as input to the model\n",
    "        cur_unix_timestamp = datetime.datetime(year=2024, month=4, day=15).timestamp()\n",
    "\n",
    "        for ticker in tickers:\n",
    "            ordered_time_series = get_time_series(ticker, data_dir, normalize=True)\n",
    "\n",
    "            if test:\n",
    "                idx = range(len(ordered_time_series) - lag_days - test_days, len(ordered_time_series) - lag_days - test_days + 1)\n",
    "            else:\n",
    "                idx = range(lag_days, len(ordered_time_series) - lag_days - test_days)\n",
    "\n",
    "            # need to get lag_days days of input tokens and predict the lag_days+1 day, and so on\n",
    "            for cur_target_idx in idx:\n",
    "\n",
    "                # get target values\n",
    "                cur_target_actual_after = [ticker_to_num[ticker]]\n",
    "                for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                    val = ordered_time_series[cur_target_idx][key]\n",
    "                    if key == 'date':\n",
    "                        val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                    cur_target_actual_after.append(val)\n",
    "\n",
    "                # each \"sample\" (input) will have multiple tokens. Each token is a vector of the day's values\n",
    "                cur_input = []\n",
    "\n",
    "                for i in range(cur_target_idx - lag_days, cur_target_idx):\n",
    "\n",
    "                    cur_token = [ticker_to_num[ticker]]\n",
    "                    for key in ordered_time_series[cur_target_idx]:\n",
    "\n",
    "                        val = ordered_time_series[cur_target_idx][key]\n",
    "                        if key == 'date':\n",
    "                            val = (datetime.datetime.strptime(val, \"%Y-%m-%d\").timestamp()) / cur_unix_timestamp  # scale to small value\n",
    "                        cur_token.append(val)\n",
    "\n",
    "                    cur_input.append(cur_token)\n",
    "\n",
    "                self.X.append(cur_input)\n",
    "                self.y.append(cur_target_actual_after)\n",
    "                self.tickers.append(ticker)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32), self.tickers[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding for Transformer (batch first)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class BigTransformer(nn.Module):\n",
    "    def __init__(self, indim, outdim, hidden_dim=256, d_model=64, nhead=4, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super(BigTransformer, self).__init__()\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(indim, hidden_dim),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "            )\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, batch_first=True)\n",
    "\n",
    "        self.decoder_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, outdim)\n",
    "        )\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "    # src: (B, S, F) batches, sequence length, features\n",
    "    # tgt: (B, F)\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.embedding(tgt.unsqueeze(1))\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src.shape[1]).to(device)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.shape[1]).to(device)\n",
    "        out = self.transformer(src, tgt, src_mask, tgt_mask, src_is_causal=True, tgt_is_causal=True)\n",
    "        out = self.decoder_mlp(out)\n",
    "        return out.squeeze(1)\n",
    "    \n",
    "    def generate(self, src, max_len=100, start_token=None, end_token=None):\n",
    "        device = src.device\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src.shape[1]).to(device)\n",
    "\n",
    "        output_seq = torch.zeros((src.shape[0], 1, self.d_model)).to(device)\n",
    "        if start_token is not None:\n",
    "            output_seq[:, 0] = self.embedding(start_token.to(device))\n",
    "\n",
    "        for i in range(max_len):\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(output_seq.shape[1]).to(device)\n",
    "            next_token = self.transformer(src, output_seq, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "            output_seq = torch.cat([output_seq, next_token], dim=1)\n",
    "\n",
    "            if end_token is not None and torch.all(next_token == end_token):\n",
    "                break\n",
    "            \n",
    "        output_seq = self.decoder_mlp(output_seq[:, 1:]) # exclude start token\n",
    "        \n",
    "        return output_seq.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public methods are constructor, train, evaluate, and load_model\n",
    "class BigTransformerAgent:\n",
    "    def __init__(self, indim, outdim, hidden_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 device, checkpoint_dir, init_lr, lr_decay, min_lr, decay_lr_every):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        self.allow_training = True\n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.device = device\n",
    "        self.model = BigTransformer(indim, outdim, hidden_dim, d_model, nhead, \n",
    "                                    num_encoder_layers, num_decoder_layers)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=init_lr)\n",
    "\n",
    "        # at epoch e, lr will be init_lr * scheduler_lamb(e)\n",
    "        scheduler_lamb = lambda epoch: max(lr_decay ** (epoch // decay_lr_every), min_lr / init_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, scheduler_lamb)\n",
    "\n",
    "        self.criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cur_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        self.allow_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def __save_model(self):\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "\n",
    "        dt = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(self.checkpoint_dir, f\"{dt}_agent_{self.cur_epoch}.pth\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"epoch\": self.cur_epoch\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def __save_returns(self, returns):\n",
    "        returns_prefix = f\"{self.checkpoint_dir}/returns\"\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        if not os.path.isdir(returns_prefix):\n",
    "            os.mkdir(returns_prefix)\n",
    "\n",
    "        np_filename = f\"{returns_prefix}/{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.npy\"\n",
    "        np.save(np_filename, returns)\n",
    "\n",
    "        return np_filename\n",
    "\n",
    "    def train(self, dataloader, epochs):\n",
    "        if not self.allow_training:\n",
    "            raise Exception(\"Not allowed to train after loading\")\n",
    "\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for (X, y, tickers) in dataloader:\n",
    "                X = X.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                y_pred = self.model(X, y)\n",
    "\n",
    "                loss = self.criterion(y, y_pred)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # TODO is this good?\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # self.scheduler.step()\n",
    "\n",
    "            losses.append(epoch_loss)\n",
    "\n",
    "            self.cur_epoch += 1\n",
    "\n",
    "            if self.cur_epoch % 25 == 0:\n",
    "                self.__save_model()\n",
    "\n",
    "            average_loss = epoch_loss / len(dataloader.dataset)  # epoch loss is the sum of each sample's loss since mse reduction is sum\n",
    "            print(f\"Epoch {self.cur_epoch}, Loss: {epoch_loss:.4f}; Average Loss: {average_loss}; lr: {self.scheduler.get_last_lr()}\")\n",
    "\n",
    "        self.model.eval()\n",
    "        return self.__save_returns(losses)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        gt = torch.Tensor([])\n",
    "        preds = torch.Tensor([])\n",
    "        companies = []\n",
    "        for batch, (X, y, co) in enumerate(dataloader):\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            y_pred = self.model.generate(X, max_len=1)\n",
    "\n",
    "            loss = self.criterion(y, y_pred)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            gt = torch.concat([gt, y], dim=0)\n",
    "            preds = torch.concat([preds, y_pred], dim=0)\n",
    "            companies.extend(co)\n",
    "\n",
    "        print(f\"Average Loss: {total_loss / len(dataloader.dataset)}\")  # total loss is the sum of each sample's loss since mse reduction is sum\n",
    "\n",
    "        gt = gt.to('cpu')\n",
    "        preds = preds.to('cpu')\n",
    "\n",
    "        return gt, preds, companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public methods are constructor, train, evaluate, and load_model\n",
    "# agent for incremental learning models\n",
    "class IncrementalAgent:\n",
    "    def __init__(self, indim, outdim, hidden_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 device, checkpoint_dir, init_lr, lr_decay, min_lr, decay_lr_every, tickers):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.tickers = tickers\n",
    "\n",
    "        self.allow_training = True\n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.device = device\n",
    "        self.model = BigTransformer(indim, outdim, hidden_dim, d_model, nhead, \n",
    "                                    num_encoder_layers, num_decoder_layers)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=init_lr)\n",
    "\n",
    "        # at epoch e, lr will be init_lr * scheduler_lamb(e)\n",
    "        scheduler_lamb = lambda epoch: max(lr_decay ** (epoch // decay_lr_every), min_lr / init_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, scheduler_lamb)\n",
    "\n",
    "        self.criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cur_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        self.allow_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def __save_model(self):\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "\n",
    "        dt = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(self.checkpoint_dir, f\"{dt}_agent_{self.cur_epoch}.pth\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"epoch\": self.cur_epoch\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def __save_returns(self, fn, returns):\n",
    "        returns_prefix = f\"{self.checkpoint_dir}/returns\"\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        if not os.path.isdir(returns_prefix):\n",
    "            os.mkdir(returns_prefix)\n",
    "\n",
    "        np_filename = f\"{returns_prefix}/{fn}{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.npy\"\n",
    "        np.save(np_filename, returns)\n",
    "\n",
    "        return np_filename\n",
    "\n",
    "\n",
    "    def train(self, test_days, epochs):\n",
    "        if not self.allow_training:\n",
    "            raise Exception(\"Not allowed to train after loading\")\n",
    "        \n",
    "\n",
    "        losses = []\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        companies = []\n",
    "\n",
    "        for day in range(test_days):\n",
    "            # create new datasets for each new test_day\n",
    "            train_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, False)\n",
    "            test_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, True)\n",
    "            train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "            test_loader = DataLoader(test_set)\n",
    "\n",
    "            losses_per_day = []\n",
    "            self.model.train()\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for (X, y, tickers) in train_loader:\n",
    "                    X = X.to(self.device)\n",
    "                    y = y.to(self.device)\n",
    "\n",
    "                    y_pred = self.model(X, y)\n",
    "\n",
    "                    loss = self.criterion(y_pred, y)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # TODO is this good?\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                # self.scheduler.step()\n",
    "\n",
    "                losses_per_day.append(epoch_loss)\n",
    "\n",
    "                self.cur_epoch += 1\n",
    "\n",
    "                if self.cur_epoch % 25 == 0:\n",
    "                    self.__save_model()\n",
    "\n",
    "                average_loss = epoch_loss / len(train_set)  # epoch loss is the sum of each sample's loss since mse reduction is sum\n",
    "                print(f\"Test Day {day}, Epoch {self.cur_epoch}, Loss: {epoch_loss:.4f}; Average Loss: {average_loss}; lr: {self.scheduler.get_last_lr()}\")\n",
    "\n",
    "            gt, pred, tickers = self.evaluate(test_loader)\n",
    "            ground_truths.append(gt)\n",
    "            predictions.append(pred)\n",
    "            companies.extend(tickers)\n",
    "\n",
    "            losses.append(losses_per_day)\n",
    "\n",
    "        ground_truths = np.array(ground_truths)\n",
    "        predictions = np.array(ground_truths)\n",
    "\n",
    "        self.__save_returns('ground_truths_', ground_truths)\n",
    "        self.__save_returns('predictions', predictions)\n",
    "\n",
    "        self.model.eval()\n",
    "        return ground_truths, predictions, companies\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        gt = torch.Tensor([])\n",
    "        preds = torch.Tensor([])\n",
    "        companies = []\n",
    "        for batch, (X, y, co) in enumerate(dataloader):\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            y_pred = self.model.generate(X, max_len=1)\n",
    "\n",
    "            loss = self.criterion(y_pred, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            gt = torch.concat([gt, y], dim=0)\n",
    "            preds = torch.concat([preds, y_pred], dim=0)\n",
    "            companies.extend(co)\n",
    "\n",
    "        print(f\"Test Loss: {total_loss / len(dataloader.dataset)}\")  # total loss is the sum of each sample's loss since mse reduction is sum\n",
    "\n",
    "        gt = gt.to('cpu').detach().numpy()\n",
    "        preds = preds.to('cpu').detach().numpy()\n",
    "\n",
    "        return gt, preds, companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public methods are constructor, train, evaluate, and load_model\n",
    "# agent for incremental learning models\n",
    "class IncrementalProbAgent:\n",
    "    def __init__(self, indim, outdim, hidden_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 device, checkpoint_dir, init_lr, lr_decay, min_lr, decay_lr_every, tickers):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.tickers = tickers\n",
    "\n",
    "        self.allow_training = True\n",
    "        self.cur_epoch = 0\n",
    "\n",
    "        self.device = device\n",
    "        self.model = BigTransformer(indim, outdim, hidden_dim, d_model, nhead, \n",
    "                                    num_encoder_layers, num_decoder_layers)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=init_lr)\n",
    "\n",
    "        # at epoch e, lr will be init_lr * scheduler_lamb(e)\n",
    "        scheduler_lamb = lambda epoch: max(lr_decay ** (epoch // decay_lr_every), min_lr / init_lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, scheduler_lamb)\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cur_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        self.allow_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "    def __save_model(self):\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "\n",
    "        dt = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "        path = os.path.join(self.checkpoint_dir, f\"{dt}_agent_{self.cur_epoch}.pth\")\n",
    "\n",
    "        params = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"epoch\": self.cur_epoch\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def __save_returns(self, fn, returns):\n",
    "        returns_prefix = f\"{self.checkpoint_dir}/returns\"\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            os.mkdir(self.checkpoint_dir)\n",
    "        if not os.path.isdir(returns_prefix):\n",
    "            os.mkdir(returns_prefix)\n",
    "\n",
    "        np_filename = f\"{returns_prefix}/{fn}{datetime.datetime.now().strftime('%m-%d_%H-%M-%S')}.npy\"\n",
    "        np.save(np_filename, returns)\n",
    "\n",
    "        return np_filename\n",
    "\n",
    "    def train(self, test_days, epochs):\n",
    "        if not self.allow_training:\n",
    "            raise Exception(\"Not allowed to train after loading\")\n",
    "        \n",
    "        losses = []\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        companies = []\n",
    "\n",
    "        for day in range(test_days):\n",
    "            # create new datasets for each new test_day\n",
    "            train_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, False)\n",
    "            test_set = IncrementalDataset(self.tickers, '../data_combined', 30, test_days - day, True)\n",
    "            train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "            test_loader = DataLoader(test_set)\n",
    "\n",
    "            losses_per_day = []\n",
    "            self.model.train()\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for (X, y, tickers) in train_loader:\n",
    "                    X = X.to(self.device)\n",
    "                    y = y.to(self.device)\n",
    "\n",
    "                    y_pred = self.model(X, y) # y_pred will be (B, 1)\n",
    "\n",
    "                    y = y[:,7].clone() # pct_change is 7th column\n",
    "                    y[y <= 0] = 0\n",
    "                    y[y > 0] = 1\n",
    "\n",
    "                    loss = self.criterion(y_pred, y.unsqueeze(1))\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # TODO is this good?\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                # self.scheduler.step()\n",
    "\n",
    "                losses_per_day.append(epoch_loss)\n",
    "\n",
    "                self.cur_epoch += 1\n",
    "\n",
    "                if self.cur_epoch % 25 == 0:\n",
    "                    self.__save_model()\n",
    "\n",
    "                average_loss = epoch_loss / len(train_set)  # epoch loss is the sum of each sample's loss since mse reduction is sum\n",
    "                print(f\"Test Day {day}, Epoch {self.cur_epoch}, Loss: {epoch_loss:.4f}; Average Loss: {average_loss}; lr: {self.scheduler.get_last_lr()}\")\n",
    "\n",
    "            gt, pred, tickers = self.evaluate(test_loader)\n",
    "            ground_truths.append(gt)\n",
    "            predictions.append(pred)\n",
    "            companies.extend(tickers)\n",
    "\n",
    "            losses.append(losses_per_day)\n",
    "\n",
    "        ground_truths = np.array(ground_truths).squeeze()\n",
    "        predictions = np.array(predictions).squeeze()\n",
    "\n",
    "        self.__save_returns('ground_truths_', ground_truths)\n",
    "        self.__save_returns('predictions', predictions)\n",
    "\n",
    "        self.model.eval()\n",
    "        return ground_truths, predictions, companies\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        gt = torch.Tensor([]).to(self.device)\n",
    "        preds = torch.Tensor([]).to(self.device)\n",
    "        companies = []\n",
    "        for batch, (X, y, co) in enumerate(dataloader):\n",
    "            X = X.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            y_pred = self.model.generate(X, max_len=1)\n",
    "\n",
    "            y = y[:,7].clone() # pct_change is 7th column\n",
    "            y[y <= 0] = 0\n",
    "            y[y > 0] = 1\n",
    "\n",
    "            loss = self.criterion(y_pred, y.unsqueeze(1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            gt = torch.concat([gt, y.unsqueeze(1)], dim=0)\n",
    "            preds = torch.concat([preds, torch.sigmoid(y_pred)], dim=0)\n",
    "            print(torch.sigmoid(y_pred), y_pred)\n",
    "            companies.extend(co)\n",
    "\n",
    "        print(f\"Test Loss: {total_loss / len(dataloader.dataset)}\")  # total loss is the sum of each sample's loss since mse reduction is sum\n",
    "\n",
    "        gt = gt.to('cpu').detach().numpy()\n",
    "        preds = preds.to('cpu').detach().numpy()\n",
    "\n",
    "        return gt, preds, companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "tickers = [\"AAPL\", \"NVDA\", 'MSFT', 'TSLA', 'AMZN']\n",
    "data_dir = \"../data_combined/\"\n",
    "test_days = 100\n",
    "lag_days = 30\n",
    "batch_size = 128\n",
    "\n",
    "# model params\n",
    "indim = 23\n",
    "outdim = 1\n",
    "hidden_dim = 256\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = VariableLengthDataset(tickers=tickers, data_dir=data_dir, lag_days=lag_days, test_days=test_days, test=False)\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test = VariableLengthDataset(tickers=tickers, data_dir=data_dir, lag_days=lag_days, test_days=test_days, test=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BigTransformerAgent(indim=23, outdim=23, hidden_dim=256, d_model=64, nhead=8, num_encoder_layers=6, num_decoder_layers=6, device=device, \n",
    "                            checkpoint_dir='../saved_models/23outdimMSEmodel', init_lr=0.001, lr_decay=0.0001, min_lr=0.0000025, decay_lr_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(train_loader, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = IncrementalAgent(indim=23, outdim=23, hidden_dim=256, d_model=64, nhead=8, num_encoder_layers=6, num_decoder_layers=6, device=device, \n",
    "                            checkpoint_dir='../saved_models/23outdimIncrementalMSEmodel', init_lr=0.0001, lr_decay=0.5, min_lr=0.00000025, decay_lr_every=10, \n",
    "                            tickers=tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, preds, companies = agent.train(test_days, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = IncrementalProbAgent(indim=23, outdim=1, hidden_dim=256, d_model=64, nhead=8, num_encoder_layers=6, num_decoder_layers=6, device=device, \n",
    "                            checkpoint_dir='../saved_models/1outdimIncrementalMSEmodel', init_lr=0.0001, lr_decay=0.5, min_lr=0.00000025, decay_lr_every=10, \n",
    "                            tickers=tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predicted_scores, companies = agent.train(5, 1) # one column for each company ticker, flatten to compute ROC over all\n",
    "fpr, tpr, thresholds = roc_curve(true_labels.reshape(-1), predicted_scores.reshape(-1))\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predicted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
